{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6376419e",
      "metadata": {
        "id": "6376419e"
      },
      "source": [
        "# 实验任务二：使用CNN来进行图像分类\n",
        "## CIFAR-10 数据集\n",
        "本次实验使用CIFAR-10 数据集来进行实验。\n",
        "CIFAR-10 数据集包含 60,000 张 32×32 像素的彩色图像，\n",
        "分为 10 个类别，每个类别有 6,000 张图像。\n",
        "具体类别包括飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。\n",
        "数据集被分为训练集和测试集，\n",
        "其中训练集包含 50,000 张图像，测试集包含 10,000 张图像。\n",
        "## 1. 在CIFAR数据集上实现CNN\n",
        "本次任务要求补全代码中空缺部分，包括实现一个CNN类，以及训练过程代码\n",
        "\n",
        "数据集下载链接：\n",
        "\n",
        "https://box.nju.edu.cn/f/d59d5d910d754c3091f5/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c7434d95",
      "metadata": {
        "id": "c7434d95"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81602cb6",
      "metadata": {
        "id": "81602cb6"
      },
      "source": [
        "导入CIFAR-10数据集："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8fd51464",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fd51464",
        "outputId": "f95d5c5e-3047-433f-ab7a-a8a4195b1d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 47.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# 下载并加载训练集\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# 创建数据加载器\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset,\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset,\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82c307b5",
      "metadata": {
        "id": "82c307b5"
      },
      "source": [
        "定义CNN网络："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a4c80fdb",
      "metadata": {
        "id": "a4c80fdb"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        #TODO: 实现模型结构\n",
        "        #TODO 实现self.conv1:卷积层\n",
        "        self.conv1 = nn.Conv2d(3,32, kernel_size=3, padding=1)\n",
        "        #TODO 实现self.conv2:卷积层\n",
        "        self.conv2 = nn.Conv2d(32,64, kernel_size=3, padding=1)\n",
        "        #TODO 实现self.pool: MaxPool2d\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        #TODO 实现self.fc1: 线性层\n",
        "        self.fc1 = nn.Linear(64*8*8,500)\n",
        "        #TODO 实现self.fc2：线性层\n",
        "        self.fc2 = nn.Linear(500,10)\n",
        "        #TODO 实现 self.dropout: Dropout层\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c555e410",
      "metadata": {
        "id": "c555e410"
      },
      "source": [
        "进行训练："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d47923b3",
      "metadata": {
        "id": "d47923b3"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, test_loader, device):\n",
        "    num_epochs = 15\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            #TODO:实现训练部分，完成反向传播过程\n",
        "            #TODO: optimizer梯度清除\n",
        "            optimizer.zero_grad()\n",
        "            #TODO: 模型输入\n",
        "            outputs = model(inputs)\n",
        "            #TODO: 计算损失\n",
        "            loss = criterion(outputs, labels).sum()\n",
        "            #TODO: 反向传播\n",
        "            loss.backward()\n",
        "            #TODO: 更新参数\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:  # 每100个batch打印一次损失\n",
        "                print(\n",
        "                  f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "        #每个epoch结束后在测试集上评估模型\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7e70fb38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e70fb38",
        "outputId": "fcdcfb23-9f84-4299-946f-7f7f17976d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15], Step [100/1563], Loss: 1.9610\n",
            "Epoch [1/15], Step [200/1563], Loss: 1.5819\n",
            "Epoch [1/15], Step [300/1563], Loss: 1.4527\n",
            "Epoch [1/15], Step [400/1563], Loss: 1.3865\n",
            "Epoch [1/15], Step [500/1563], Loss: 1.3535\n",
            "Epoch [1/15], Step [600/1563], Loss: 1.2826\n",
            "Epoch [1/15], Step [700/1563], Loss: 1.2283\n",
            "Epoch [1/15], Step [800/1563], Loss: 1.1969\n",
            "Epoch [1/15], Step [900/1563], Loss: 1.1606\n",
            "Epoch [1/15], Step [1000/1563], Loss: 1.1399\n",
            "Epoch [1/15], Step [1100/1563], Loss: 1.1619\n",
            "Epoch [1/15], Step [1200/1563], Loss: 1.1164\n",
            "Epoch [1/15], Step [1300/1563], Loss: 1.1098\n",
            "Epoch [1/15], Step [1400/1563], Loss: 1.0721\n",
            "Epoch [1/15], Step [1500/1563], Loss: 1.0497\n",
            "Test Accuracy: 63.89%\n",
            "Epoch [2/15], Step [100/1563], Loss: 0.9338\n",
            "Epoch [2/15], Step [200/1563], Loss: 0.9477\n",
            "Epoch [2/15], Step [300/1563], Loss: 0.9458\n",
            "Epoch [2/15], Step [400/1563], Loss: 0.9555\n",
            "Epoch [2/15], Step [500/1563], Loss: 0.9217\n",
            "Epoch [2/15], Step [600/1563], Loss: 0.8886\n",
            "Epoch [2/15], Step [700/1563], Loss: 0.8842\n",
            "Epoch [2/15], Step [800/1563], Loss: 0.8535\n",
            "Epoch [2/15], Step [900/1563], Loss: 0.8831\n",
            "Epoch [2/15], Step [1000/1563], Loss: 0.8827\n",
            "Epoch [2/15], Step [1100/1563], Loss: 0.8875\n",
            "Epoch [2/15], Step [1200/1563], Loss: 0.8531\n",
            "Epoch [2/15], Step [1300/1563], Loss: 0.8832\n",
            "Epoch [2/15], Step [1400/1563], Loss: 0.8736\n",
            "Epoch [2/15], Step [1500/1563], Loss: 0.8728\n",
            "Test Accuracy: 68.81%\n",
            "Epoch [3/15], Step [100/1563], Loss: 0.7404\n",
            "Epoch [3/15], Step [200/1563], Loss: 0.6924\n",
            "Epoch [3/15], Step [300/1563], Loss: 0.7136\n",
            "Epoch [3/15], Step [400/1563], Loss: 0.7030\n",
            "Epoch [3/15], Step [500/1563], Loss: 0.6810\n",
            "Epoch [3/15], Step [600/1563], Loss: 0.6891\n",
            "Epoch [3/15], Step [700/1563], Loss: 0.7495\n",
            "Epoch [3/15], Step [800/1563], Loss: 0.7566\n",
            "Epoch [3/15], Step [900/1563], Loss: 0.7171\n",
            "Epoch [3/15], Step [1000/1563], Loss: 0.6705\n",
            "Epoch [3/15], Step [1100/1563], Loss: 0.7089\n",
            "Epoch [3/15], Step [1200/1563], Loss: 0.7538\n",
            "Epoch [3/15], Step [1300/1563], Loss: 0.7252\n",
            "Epoch [3/15], Step [1400/1563], Loss: 0.7519\n",
            "Epoch [3/15], Step [1500/1563], Loss: 0.7421\n",
            "Test Accuracy: 71.22%\n",
            "Epoch [4/15], Step [100/1563], Loss: 0.5407\n",
            "Epoch [4/15], Step [200/1563], Loss: 0.5583\n",
            "Epoch [4/15], Step [300/1563], Loss: 0.5568\n",
            "Epoch [4/15], Step [400/1563], Loss: 0.5628\n",
            "Epoch [4/15], Step [500/1563], Loss: 0.5971\n",
            "Epoch [4/15], Step [600/1563], Loss: 0.5699\n",
            "Epoch [4/15], Step [700/1563], Loss: 0.5586\n",
            "Epoch [4/15], Step [800/1563], Loss: 0.5743\n",
            "Epoch [4/15], Step [900/1563], Loss: 0.5557\n",
            "Epoch [4/15], Step [1000/1563], Loss: 0.5943\n",
            "Epoch [4/15], Step [1100/1563], Loss: 0.5644\n",
            "Epoch [4/15], Step [1200/1563], Loss: 0.5954\n",
            "Epoch [4/15], Step [1300/1563], Loss: 0.5616\n",
            "Epoch [4/15], Step [1400/1563], Loss: 0.5540\n",
            "Epoch [4/15], Step [1500/1563], Loss: 0.5941\n",
            "Test Accuracy: 72.33%\n",
            "Epoch [5/15], Step [100/1563], Loss: 0.3966\n",
            "Epoch [5/15], Step [200/1563], Loss: 0.4226\n",
            "Epoch [5/15], Step [300/1563], Loss: 0.4148\n",
            "Epoch [5/15], Step [400/1563], Loss: 0.4199\n",
            "Epoch [5/15], Step [500/1563], Loss: 0.4380\n",
            "Epoch [5/15], Step [600/1563], Loss: 0.4347\n",
            "Epoch [5/15], Step [700/1563], Loss: 0.4176\n",
            "Epoch [5/15], Step [800/1563], Loss: 0.4534\n",
            "Epoch [5/15], Step [900/1563], Loss: 0.4561\n",
            "Epoch [5/15], Step [1000/1563], Loss: 0.4683\n",
            "Epoch [5/15], Step [1100/1563], Loss: 0.4392\n",
            "Epoch [5/15], Step [1200/1563], Loss: 0.4646\n",
            "Epoch [5/15], Step [1300/1563], Loss: 0.4893\n",
            "Epoch [5/15], Step [1400/1563], Loss: 0.4597\n",
            "Epoch [5/15], Step [1500/1563], Loss: 0.4715\n",
            "Test Accuracy: 73.26%\n",
            "Epoch [6/15], Step [100/1563], Loss: 0.3043\n",
            "Epoch [6/15], Step [200/1563], Loss: 0.2832\n",
            "Epoch [6/15], Step [300/1563], Loss: 0.3042\n",
            "Epoch [6/15], Step [400/1563], Loss: 0.3205\n",
            "Epoch [6/15], Step [500/1563], Loss: 0.3300\n",
            "Epoch [6/15], Step [600/1563], Loss: 0.3100\n",
            "Epoch [6/15], Step [700/1563], Loss: 0.3570\n",
            "Epoch [6/15], Step [800/1563], Loss: 0.3424\n",
            "Epoch [6/15], Step [900/1563], Loss: 0.3447\n",
            "Epoch [6/15], Step [1000/1563], Loss: 0.3541\n",
            "Epoch [6/15], Step [1100/1563], Loss: 0.3864\n",
            "Epoch [6/15], Step [1200/1563], Loss: 0.3436\n",
            "Epoch [6/15], Step [1300/1563], Loss: 0.3728\n",
            "Epoch [6/15], Step [1400/1563], Loss: 0.3394\n",
            "Epoch [6/15], Step [1500/1563], Loss: 0.3606\n",
            "Test Accuracy: 72.76%\n",
            "Epoch [7/15], Step [100/1563], Loss: 0.2155\n",
            "Epoch [7/15], Step [200/1563], Loss: 0.2000\n",
            "Epoch [7/15], Step [300/1563], Loss: 0.2334\n",
            "Epoch [7/15], Step [400/1563], Loss: 0.2063\n",
            "Epoch [7/15], Step [500/1563], Loss: 0.2537\n",
            "Epoch [7/15], Step [600/1563], Loss: 0.2369\n",
            "Epoch [7/15], Step [700/1563], Loss: 0.2655\n",
            "Epoch [7/15], Step [800/1563], Loss: 0.2713\n",
            "Epoch [7/15], Step [900/1563], Loss: 0.2534\n",
            "Epoch [7/15], Step [1000/1563], Loss: 0.2882\n",
            "Epoch [7/15], Step [1100/1563], Loss: 0.2558\n",
            "Epoch [7/15], Step [1200/1563], Loss: 0.2492\n",
            "Epoch [7/15], Step [1300/1563], Loss: 0.2783\n",
            "Epoch [7/15], Step [1400/1563], Loss: 0.3303\n",
            "Epoch [7/15], Step [1500/1563], Loss: 0.2834\n",
            "Test Accuracy: 72.37%\n",
            "Epoch [8/15], Step [100/1563], Loss: 0.1844\n",
            "Epoch [8/15], Step [200/1563], Loss: 0.1580\n",
            "Epoch [8/15], Step [300/1563], Loss: 0.1703\n",
            "Epoch [8/15], Step [400/1563], Loss: 0.1754\n",
            "Epoch [8/15], Step [500/1563], Loss: 0.1991\n",
            "Epoch [8/15], Step [600/1563], Loss: 0.2047\n",
            "Epoch [8/15], Step [700/1563], Loss: 0.2166\n",
            "Epoch [8/15], Step [800/1563], Loss: 0.2229\n",
            "Epoch [8/15], Step [900/1563], Loss: 0.2005\n",
            "Epoch [8/15], Step [1000/1563], Loss: 0.2201\n",
            "Epoch [8/15], Step [1100/1563], Loss: 0.2107\n",
            "Epoch [8/15], Step [1200/1563], Loss: 0.2204\n",
            "Epoch [8/15], Step [1300/1563], Loss: 0.2326\n",
            "Epoch [8/15], Step [1400/1563], Loss: 0.2449\n",
            "Epoch [8/15], Step [1500/1563], Loss: 0.2296\n",
            "Test Accuracy: 73.50%\n",
            "Epoch [9/15], Step [100/1563], Loss: 0.1352\n",
            "Epoch [9/15], Step [200/1563], Loss: 0.1360\n",
            "Epoch [9/15], Step [300/1563], Loss: 0.1631\n",
            "Epoch [9/15], Step [400/1563], Loss: 0.1633\n",
            "Epoch [9/15], Step [500/1563], Loss: 0.1663\n",
            "Epoch [9/15], Step [600/1563], Loss: 0.1709\n",
            "Epoch [9/15], Step [700/1563], Loss: 0.1710\n",
            "Epoch [9/15], Step [800/1563], Loss: 0.1759\n",
            "Epoch [9/15], Step [900/1563], Loss: 0.1610\n",
            "Epoch [9/15], Step [1000/1563], Loss: 0.1981\n",
            "Epoch [9/15], Step [1100/1563], Loss: 0.1882\n",
            "Epoch [9/15], Step [1200/1563], Loss: 0.2003\n",
            "Epoch [9/15], Step [1300/1563], Loss: 0.1812\n",
            "Epoch [9/15], Step [1400/1563], Loss: 0.2106\n",
            "Epoch [9/15], Step [1500/1563], Loss: 0.2108\n",
            "Test Accuracy: 71.71%\n",
            "Epoch [10/15], Step [100/1563], Loss: 0.1232\n",
            "Epoch [10/15], Step [200/1563], Loss: 0.1217\n",
            "Epoch [10/15], Step [300/1563], Loss: 0.1338\n",
            "Epoch [10/15], Step [400/1563], Loss: 0.1252\n",
            "Epoch [10/15], Step [500/1563], Loss: 0.1213\n",
            "Epoch [10/15], Step [600/1563], Loss: 0.1494\n",
            "Epoch [10/15], Step [700/1563], Loss: 0.1774\n",
            "Epoch [10/15], Step [800/1563], Loss: 0.1720\n",
            "Epoch [10/15], Step [900/1563], Loss: 0.1519\n",
            "Epoch [10/15], Step [1000/1563], Loss: 0.1444\n",
            "Epoch [10/15], Step [1100/1563], Loss: 0.1690\n",
            "Epoch [10/15], Step [1200/1563], Loss: 0.1495\n",
            "Epoch [10/15], Step [1300/1563], Loss: 0.1467\n",
            "Epoch [10/15], Step [1400/1563], Loss: 0.1620\n",
            "Epoch [10/15], Step [1500/1563], Loss: 0.1624\n",
            "Test Accuracy: 72.37%\n",
            "Epoch [11/15], Step [100/1563], Loss: 0.1154\n",
            "Epoch [11/15], Step [200/1563], Loss: 0.1304\n",
            "Epoch [11/15], Step [300/1563], Loss: 0.0967\n",
            "Epoch [11/15], Step [400/1563], Loss: 0.1071\n",
            "Epoch [11/15], Step [500/1563], Loss: 0.1339\n",
            "Epoch [11/15], Step [600/1563], Loss: 0.1371\n",
            "Epoch [11/15], Step [700/1563], Loss: 0.1359\n",
            "Epoch [11/15], Step [800/1563], Loss: 0.1394\n",
            "Epoch [11/15], Step [900/1563], Loss: 0.1530\n",
            "Epoch [11/15], Step [1000/1563], Loss: 0.1485\n",
            "Epoch [11/15], Step [1100/1563], Loss: 0.1453\n",
            "Epoch [11/15], Step [1200/1563], Loss: 0.1390\n",
            "Epoch [11/15], Step [1300/1563], Loss: 0.1729\n",
            "Epoch [11/15], Step [1400/1563], Loss: 0.1536\n",
            "Epoch [11/15], Step [1500/1563], Loss: 0.1524\n",
            "Test Accuracy: 73.24%\n",
            "Epoch [12/15], Step [100/1563], Loss: 0.1082\n",
            "Epoch [12/15], Step [200/1563], Loss: 0.1008\n",
            "Epoch [12/15], Step [300/1563], Loss: 0.1025\n",
            "Epoch [12/15], Step [400/1563], Loss: 0.1070\n",
            "Epoch [12/15], Step [500/1563], Loss: 0.1183\n",
            "Epoch [12/15], Step [600/1563], Loss: 0.0969\n",
            "Epoch [12/15], Step [700/1563], Loss: 0.1066\n",
            "Epoch [12/15], Step [800/1563], Loss: 0.1150\n",
            "Epoch [12/15], Step [900/1563], Loss: 0.1408\n",
            "Epoch [12/15], Step [1000/1563], Loss: 0.1523\n",
            "Epoch [12/15], Step [1100/1563], Loss: 0.1441\n",
            "Epoch [12/15], Step [1200/1563], Loss: 0.1164\n",
            "Epoch [12/15], Step [1300/1563], Loss: 0.1283\n",
            "Epoch [12/15], Step [1400/1563], Loss: 0.1230\n",
            "Epoch [12/15], Step [1500/1563], Loss: 0.1463\n",
            "Test Accuracy: 72.66%\n",
            "Epoch [13/15], Step [100/1563], Loss: 0.1036\n",
            "Epoch [13/15], Step [200/1563], Loss: 0.0979\n",
            "Epoch [13/15], Step [300/1563], Loss: 0.0776\n",
            "Epoch [13/15], Step [400/1563], Loss: 0.0869\n",
            "Epoch [13/15], Step [500/1563], Loss: 0.1025\n",
            "Epoch [13/15], Step [600/1563], Loss: 0.1063\n",
            "Epoch [13/15], Step [700/1563], Loss: 0.0906\n",
            "Epoch [13/15], Step [800/1563], Loss: 0.1411\n",
            "Epoch [13/15], Step [900/1563], Loss: 0.1185\n",
            "Epoch [13/15], Step [1000/1563], Loss: 0.1258\n",
            "Epoch [13/15], Step [1100/1563], Loss: 0.1184\n",
            "Epoch [13/15], Step [1200/1563], Loss: 0.1461\n",
            "Epoch [13/15], Step [1300/1563], Loss: 0.1415\n",
            "Epoch [13/15], Step [1400/1563], Loss: 0.1326\n",
            "Epoch [13/15], Step [1500/1563], Loss: 0.1090\n",
            "Test Accuracy: 72.18%\n",
            "Epoch [14/15], Step [100/1563], Loss: 0.1303\n",
            "Epoch [14/15], Step [200/1563], Loss: 0.0885\n",
            "Epoch [14/15], Step [300/1563], Loss: 0.0883\n",
            "Epoch [14/15], Step [400/1563], Loss: 0.1026\n",
            "Epoch [14/15], Step [500/1563], Loss: 0.0859\n",
            "Epoch [14/15], Step [600/1563], Loss: 0.1161\n",
            "Epoch [14/15], Step [700/1563], Loss: 0.1251\n",
            "Epoch [14/15], Step [800/1563], Loss: 0.1029\n",
            "Epoch [14/15], Step [900/1563], Loss: 0.1064\n",
            "Epoch [14/15], Step [1000/1563], Loss: 0.1037\n",
            "Epoch [14/15], Step [1100/1563], Loss: 0.1283\n",
            "Epoch [14/15], Step [1200/1563], Loss: 0.1052\n",
            "Epoch [14/15], Step [1300/1563], Loss: 0.1416\n",
            "Epoch [14/15], Step [1400/1563], Loss: 0.1076\n",
            "Epoch [14/15], Step [1500/1563], Loss: 0.1224\n",
            "Test Accuracy: 72.60%\n",
            "Epoch [15/15], Step [100/1563], Loss: 0.1017\n",
            "Epoch [15/15], Step [200/1563], Loss: 0.0886\n",
            "Epoch [15/15], Step [300/1563], Loss: 0.0759\n",
            "Epoch [15/15], Step [400/1563], Loss: 0.0952\n",
            "Epoch [15/15], Step [500/1563], Loss: 0.0846\n",
            "Epoch [15/15], Step [600/1563], Loss: 0.0892\n",
            "Epoch [15/15], Step [700/1563], Loss: 0.1045\n",
            "Epoch [15/15], Step [800/1563], Loss: 0.1089\n",
            "Epoch [15/15], Step [900/1563], Loss: 0.1196\n",
            "Epoch [15/15], Step [1000/1563], Loss: 0.0986\n",
            "Epoch [15/15], Step [1100/1563], Loss: 0.0982\n",
            "Epoch [15/15], Step [1200/1563], Loss: 0.1151\n",
            "Epoch [15/15], Step [1300/1563], Loss: 0.1265\n",
            "Epoch [15/15], Step [1400/1563], Loss: 0.1047\n",
            "Epoch [15/15], Step [1500/1563], Loss: 0.1203\n",
            "Test Accuracy: 72.77%\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#创建模型\n",
        "model = SimpleCNN().to(device)\n",
        "train(model, trainloader, testloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4e8d7195",
      "metadata": {
        "id": "4e8d7195"
      },
      "outputs": [],
      "source": [
        "def denormalize(tensor):\n",
        "    # 输入是归一化后的张量 [C, H, W]\n",
        "    # 反归一化：(tensor * std) + mean\n",
        "    # 原始归一化参数：mean=0.5, std=0.5\n",
        "    return tensor * 0.5 + 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d6983f5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "d6983f5f",
        "outputId": "46130d19-9200-4102-fb21-eeb1530dd4a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "data_iter = iter(trainloader)\n",
        "images, labels = next(data_iter)  # 获取第一个batch\n",
        "\n",
        "# 反归一化并转换为numpy\n",
        "img = denormalize(images[0]).numpy()  # 取batch中的第一张\n",
        "img = np.transpose(img, (1, 2, 0))    # 从(C, H, W)转为(H, W, C)\n",
        "\n",
        "# 显示图像\n",
        "plt.imshow(img)\n",
        "plt.title(f\"Label: {trainset.classes[labels[0]]}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c654c8e",
      "metadata": {
        "id": "7c654c8e"
      },
      "source": [
        "## 2. 在MNIST数据集上实现CNN：\n",
        "在实验二中我们实现了在MNIST数据集上进行分类，使用本节的CNN又该如何实现，结合本节内容以及实验二内容尝试实现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c4b6572e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4b6572e",
        "outputId": "ab849eb2-b3f7-43c3-c113-39db33ebd04d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 39.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.24MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.14MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.76MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "# batch_size=64表示每个批次包含64个样本。可以根据硬件（如内存/GPU显存）调整这个值。\n",
        "# shuffle=True表示在每个epoch开始时都将训练数据集打乱顺序。\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "#对于测试集我们通常不需要打乱顺序\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f45a9892",
      "metadata": {
        "id": "f45a9892"
      },
      "outputs": [],
      "source": [
        "class MINSTCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MINSTCNN, self).__init__()\n",
        "        #TODO: 实现模型结构\n",
        "        #TODO 实现self.conv1:卷积层\n",
        "        self.conv1 = nn.Conv2d(1,16, kernel_size=3, padding=1)\n",
        "        #TODO 实现self.conv2:卷积层\n",
        "        self.conv2 = nn.Conv2d(16,16, kernel_size=3, padding=1)\n",
        "        #TODO 实现self.pool: MaxPool2d\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        #TODO 实现self.fc1: 线性层\n",
        "        self.fc1 = nn.Linear(28*28,500)\n",
        "        #TODO 实现self.fc2：线性层\n",
        "        self.fc2 = nn.Linear(500,10)\n",
        "        #TODO 实现 self.dropout: Dropout层\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def trainMINST(model, train_loader, test_loader, device):\n",
        "    num_epochs = 3\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            #TODO:实现训练部分，完成反向传播过程\n",
        "            #TODO: optimizer梯度清除\n",
        "            optimizer.zero_grad()\n",
        "            #TODO: 模型输入\n",
        "            outputs = model(inputs)\n",
        "            #TODO: 计算损失\n",
        "            loss = criterion(outputs, labels).sum()\n",
        "            #TODO: 反向传播\n",
        "            loss.backward()\n",
        "            #TODO: 更新参数\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:  # 每100个batch打印一次损失\n",
        "                print(\n",
        "                  f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "        #每个epoch结束后在测试集上评估模型\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "486bab43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "486bab43",
        "outputId": "d2ed9b91-2e5e-4813-8c4f-e8513a711b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:1538: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Step [100/938], Loss: 0.9629\n",
            "Epoch [1/3], Step [200/938], Loss: 0.3137\n",
            "Epoch [1/3], Step [300/938], Loss: 0.2045\n",
            "Epoch [1/3], Step [400/938], Loss: 0.1725\n",
            "Epoch [1/3], Step [500/938], Loss: 0.1320\n",
            "Epoch [1/3], Step [600/938], Loss: 0.1035\n",
            "Epoch [1/3], Step [700/938], Loss: 0.1083\n",
            "Epoch [1/3], Step [800/938], Loss: 0.0970\n",
            "Epoch [1/3], Step [900/938], Loss: 0.0921\n",
            "Test Accuracy: 97.99%\n",
            "Epoch [2/3], Step [100/938], Loss: 0.0747\n",
            "Epoch [2/3], Step [200/938], Loss: 0.0665\n",
            "Epoch [2/3], Step [300/938], Loss: 0.0677\n",
            "Epoch [2/3], Step [400/938], Loss: 0.0702\n",
            "Epoch [2/3], Step [500/938], Loss: 0.0569\n",
            "Epoch [2/3], Step [600/938], Loss: 0.0547\n",
            "Epoch [2/3], Step [700/938], Loss: 0.0527\n",
            "Epoch [2/3], Step [800/938], Loss: 0.0480\n",
            "Epoch [2/3], Step [900/938], Loss: 0.0642\n",
            "Test Accuracy: 98.31%\n",
            "Epoch [3/3], Step [100/938], Loss: 0.0420\n",
            "Epoch [3/3], Step [200/938], Loss: 0.0396\n",
            "Epoch [3/3], Step [300/938], Loss: 0.0420\n",
            "Epoch [3/3], Step [400/938], Loss: 0.0469\n",
            "Epoch [3/3], Step [500/938], Loss: 0.0379\n",
            "Epoch [3/3], Step [600/938], Loss: 0.0363\n",
            "Epoch [3/3], Step [700/938], Loss: 0.0483\n",
            "Epoch [3/3], Step [800/938], Loss: 0.0473\n",
            "Epoch [3/3], Step [900/938], Loss: 0.0443\n",
            "Test Accuracy: 98.53%\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#创建模型\n",
        "model = MINSTCNN().to(device)\n",
        "trainMINST(model, trainloader, testloader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68bb397a",
      "metadata": {
        "id": "68bb397a"
      },
      "source": [
        "## 3. 卷积神经网络（LeNet）\n",
        "本节将介绍LeNet，它是最早发布的卷积神经网络之一，\n",
        "因其在计算机视觉任务中的高效性能而受到广泛关注。\n",
        "这个模型是由AT&T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），\n",
        "目的是识别图像 (LeCun et al., 1998)中的手写数字。\n",
        "当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，\n",
        "这项工作代表了十多年来神经网络研究开发的成果。\n",
        "\n",
        "我们对原始模型做了一点小改动，去掉了最后一层的高斯激活。除此之外，这个网络与最初的LeNet-5一致。\n",
        "\n",
        "以下是通过实例化一个Sequential来实现LeNet代码."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5f37e1b",
      "metadata": {
        "id": "a5f37e1b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "net = nn.Sequential(\n",
        "    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
        "    nn.Linear(120, 84), nn.Sigmoid(),\n",
        "    nn.Linear(84, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b45f0139",
      "metadata": {
        "id": "b45f0139"
      },
      "source": [
        "下面，我们将一个大小为28x28 的单通道（黑白）图像通过LeNet。\n",
        "通过在每一层打印输出的形状，我们可以检查模型，以确保其操作与我们期望的图中一致"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ff87f78",
      "metadata": {
        "id": "7ff87f78",
        "outputId": "13b01849-b69d-45bf-f2a8-010e2d58282f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv2d output shape: \t torch.Size([1, 6, 28, 28])\n",
            "Sigmoid output shape: \t torch.Size([1, 6, 28, 28])\n",
            "AvgPool2d output shape: \t torch.Size([1, 6, 14, 14])\n",
            "Conv2d output shape: \t torch.Size([1, 16, 10, 10])\n",
            "Sigmoid output shape: \t torch.Size([1, 16, 10, 10])\n",
            "AvgPool2d output shape: \t torch.Size([1, 16, 5, 5])\n",
            "Flatten output shape: \t torch.Size([1, 400])\n",
            "Linear output shape: \t torch.Size([1, 120])\n",
            "Sigmoid output shape: \t torch.Size([1, 120])\n",
            "Linear output shape: \t torch.Size([1, 84])\n",
            "Sigmoid output shape: \t torch.Size([1, 84])\n",
            "Linear output shape: \t torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
        "for layer in net:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__,'output shape: \\t',X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad387f65",
      "metadata": {
        "id": "ad387f65",
        "outputId": "983c9111-8713-4452-8162-3ef4f1219534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 16, 10, 10])\n",
            "torch.Size([1, 16, 5, 5])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
        "x = net[0](x)\n",
        "x = net[1](x)\n",
        "x = net[2](x)\n",
        "x = net[3](x)\n",
        "print(x.shape)\n",
        "x = net[4](x)\n",
        "x = net[5](x)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf8a1579",
      "metadata": {
        "id": "bf8a1579"
      },
      "source": [
        "结合图片中所给出的LeNet以及给出的nn.Sequential，将前文给出的net结构以类的方式实现，并实现在\n",
        "MNIST数据集上的分类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6142353",
      "metadata": {
        "id": "b6142353",
        "outputId": "04a9c976-3e10-49ac-80ac-be9a8808220f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/3], Step [100/938], Loss: 2.3094\n",
            "Epoch [1/3], Step [200/938], Loss: 2.3012\n",
            "Epoch [1/3], Step [300/938], Loss: 1.9088\n",
            "Epoch [1/3], Step [400/938], Loss: 1.0515\n",
            "Epoch [1/3], Step [500/938], Loss: 0.7158\n",
            "Epoch [1/3], Step [600/938], Loss: 0.5331\n",
            "Epoch [1/3], Step [700/938], Loss: 0.4206\n",
            "Epoch [1/3], Step [800/938], Loss: 0.3769\n",
            "Epoch [1/3], Step [900/938], Loss: 0.3328\n",
            "Test Accuracy: 91.50%\n",
            "Epoch [2/3], Step [100/938], Loss: 0.3011\n",
            "Epoch [2/3], Step [200/938], Loss: 0.2827\n",
            "Epoch [2/3], Step [300/938], Loss: 0.2506\n",
            "Epoch [2/3], Step [400/938], Loss: 0.2378\n",
            "Epoch [2/3], Step [500/938], Loss: 0.2150\n",
            "Epoch [2/3], Step [600/938], Loss: 0.2167\n",
            "Epoch [2/3], Step [700/938], Loss: 0.2099\n",
            "Epoch [2/3], Step [800/938], Loss: 0.1890\n",
            "Epoch [2/3], Step [900/938], Loss: 0.1884\n",
            "Test Accuracy: 94.94%\n",
            "Epoch [3/3], Step [100/938], Loss: 0.1641\n",
            "Epoch [3/3], Step [200/938], Loss: 0.1644\n",
            "Epoch [3/3], Step [300/938], Loss: 0.1693\n",
            "Epoch [3/3], Step [400/938], Loss: 0.1636\n",
            "Epoch [3/3], Step [500/938], Loss: 0.1687\n",
            "Epoch [3/3], Step [600/938], Loss: 0.1377\n",
            "Epoch [3/3], Step [700/938], Loss: 0.1431\n",
            "Epoch [3/3], Step [800/938], Loss: 0.1453\n",
            "Epoch [3/3], Step [900/938], Loss: 0.1265\n",
            "Test Accuracy: 95.98%\n"
          ]
        }
      ],
      "source": [
        "class Mynet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mynet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,6, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(6,16, kernel_size=5)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc1 = nn.Linear(16* 5 *5,120)\n",
        "        self.fc2 = nn.Linear(120,84)\n",
        "        self.fc3 = nn.Linear(84,10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1,400)\n",
        "        x = F.sigmoid(self.fc1(x))\n",
        "        x = F.sigmoid(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#创建模型\n",
        "model = Mynet().to(device)\n",
        "trainMINST(model, trainloader, testloader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dca7988",
      "metadata": {
        "id": "6dca7988"
      },
      "source": [
        "## 4. 批量规范化\n",
        "训练深层神经网络是十分困难的，特别是在较短的时间内使他们收敛更加棘手。\n",
        "本节将介绍批量规范化（batch normalization） (Ioffe and Szegedy, 2015)，\n",
        "这是一种流行且有效的技术，可持续加速深层网络的收敛速度。\n",
        "\n",
        "为什么需要批量规范化层呢？让我们来回顾一下训练神经网络时出现的一些实际挑战。\n",
        "\n",
        "首先，数据预处理的方式通常会对最终结果产生巨大影响。  \n",
        "使用真实数据时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。\n",
        "直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。\n",
        "\n",
        "第二，对于典型的多层感知机或卷积神经网络。当我们训练时，中间层中的变量（\n",
        "例如，多层感知机中的仿射变换输出）\n",
        "可能具有更广的变化范围：不论是沿着从输入到输出的层，跨同一层中的单元，\n",
        "或是随着时间的推移，模型参数的随着训练更新变幻莫测。 批量规范化的发明者非正式地假设，\n",
        "这些变量分布中的这种偏移可能会阻碍网络的收敛。\n",
        "直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。\n",
        "\n",
        "第三，更深层的网络很复杂，容易过拟合。 这意味着正则化变得更加重要。\n",
        "\n",
        "批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，\n",
        "我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。\n",
        "接下来，我们应用比例系数和比例偏移。 正是由于这个基于批量统计的标准化，才有了批量规范化的名称。\n",
        "\n",
        "请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。\n",
        "这是因为在减去均值之后，每个隐藏单元将为0。 所以，\n",
        "只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。\n",
        "请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044eb612",
      "metadata": {
        "id": "044eb612"
      },
      "source": [
        "### 从零实现\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ec392a",
      "metadata": {
        "id": "83ec392a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "#from d2l import torch as d2l\n",
        "\n",
        "\n",
        "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
        "    # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式\n",
        "    if not torch.is_grad_enabled():\n",
        "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
        "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "    else:\n",
        "        assert len(X.shape) in (2, 4)\n",
        "        if len(X.shape) == 2:\n",
        "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
        "            mean = X.mean(dim=0)\n",
        "            var = ((X - mean) ** 2).mean(dim=0)\n",
        "        else:\n",
        "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。\n",
        "            # 这里我们需要保持X的形状以便后面可以做广播运算\n",
        "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
        "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
        "        # 训练模式下，用当前的均值和方差做标准化\n",
        "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
        "        # 更新移动平均的均值和方差\n",
        "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
        "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
        "    Y = gamma * X_hat + beta  # 缩放和移位\n",
        "    return Y, moving_mean.data, moving_var.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24c3d0ca",
      "metadata": {
        "id": "24c3d0ca"
      },
      "outputs": [],
      "source": [
        "class BatchNorm(nn.Module):\n",
        "    # num_features：完全连接层的输出数量或卷积层的输出通道数。\n",
        "    # num_dims：2表示完全连接层，4表示卷积层\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # 非模型参数的变量初始化为0和1\n",
        "        self.moving_mean = torch.zeros(shape)\n",
        "        self.moving_var = torch.ones(shape)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # 如果X不在内存上，将moving_mean和moving_var\n",
        "        # 复制到X所在显存上\n",
        "        if self.moving_mean.device != X.device:\n",
        "            self.moving_mean = self.moving_mean.to(X.device)\n",
        "            self.moving_var = self.moving_var.to(X.device)\n",
        "        # 保存更新过的moving_mean和moving_var\n",
        "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
        "            X, self.gamma, self.beta, self.moving_mean,\n",
        "            self.moving_var, eps=1e-5, momentum=0.9)\n",
        "        return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa66622",
      "metadata": {
        "id": "afa66622"
      },
      "source": [
        "为了更好理解如何应用BatchNorm，下面我们将其应用于LeNet模型\n",
        "回想一下，批量规范化是在卷积层或全连接层之后、相应的激活函数之前应用的."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84103ff8",
      "metadata": {
        "id": "84103ff8"
      },
      "outputs": [],
      "source": [
        "net = nn.Sequential(\n",
        "    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
        "    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),\n",
        "    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
        "    nn.Linear(84, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "105de7bf",
      "metadata": {
        "id": "105de7bf"
      },
      "source": [
        "### 简单实现\n",
        "除了使用我们刚刚定义的BatchNorm，我们也可以直接使用深度学习框架中定义的BatchNorm。\n",
        "该代码看起来几乎与我们上面的代码相同。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e091503",
      "metadata": {
        "id": "2e091503"
      },
      "outputs": [],
      "source": [
        "net = nn.Sequential(\n",
        "    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
        "    nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),\n",
        "    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),\n",
        "    nn.Linear(84, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f2b1eb3",
      "metadata": {
        "id": "0f2b1eb3"
      },
      "source": [
        "练习：使用上述定义的包含BatchNorm的LeNet网络，\n",
        "实现在MNIST数据集上的图像分类(直接使用nn.Sequential或者自定义类均可)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4beaa3f8",
      "metadata": {
        "id": "4beaa3f8",
        "outputId": "41986018-6047-4ea0-d309-1b394367ce7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/3], Step [100/938], Loss: 2.3091\n",
            "Epoch [1/3], Step [200/938], Loss: 2.3001\n",
            "Epoch [1/3], Step [300/938], Loss: 1.8815\n",
            "Epoch [1/3], Step [400/938], Loss: 1.1086\n",
            "Epoch [1/3], Step [500/938], Loss: 0.8073\n",
            "Epoch [1/3], Step [600/938], Loss: 0.6311\n",
            "Epoch [1/3], Step [700/938], Loss: 0.5443\n",
            "Epoch [1/3], Step [800/938], Loss: 0.4333\n",
            "Epoch [1/3], Step [900/938], Loss: 0.3916\n",
            "Test Accuracy: 90.58%\n",
            "Epoch [2/3], Step [100/938], Loss: 0.3173\n",
            "Epoch [2/3], Step [200/938], Loss: 0.2990\n",
            "Epoch [2/3], Step [300/938], Loss: 0.2625\n",
            "Epoch [2/3], Step [400/938], Loss: 0.2399\n",
            "Epoch [2/3], Step [500/938], Loss: 0.2385\n",
            "Epoch [2/3], Step [600/938], Loss: 0.2330\n",
            "Epoch [2/3], Step [700/938], Loss: 0.2217\n",
            "Epoch [2/3], Step [800/938], Loss: 0.2032\n",
            "Epoch [2/3], Step [900/938], Loss: 0.1988\n",
            "Test Accuracy: 94.57%\n",
            "Epoch [3/3], Step [100/938], Loss: 0.1789\n",
            "Epoch [3/3], Step [200/938], Loss: 0.1766\n",
            "Epoch [3/3], Step [300/938], Loss: 0.1638\n",
            "Epoch [3/3], Step [400/938], Loss: 0.1631\n",
            "Epoch [3/3], Step [500/938], Loss: 0.1511\n",
            "Epoch [3/3], Step [600/938], Loss: 0.1443\n",
            "Epoch [3/3], Step [700/938], Loss: 0.1306\n",
            "Epoch [3/3], Step [800/938], Loss: 0.1370\n",
            "Epoch [3/3], Step [900/938], Loss: 0.1347\n",
            "Test Accuracy: 96.05%\n"
          ]
        }
      ],
      "source": [
        "def nettrainMINST(net, train_loader, test_loader, device):\n",
        "    num_epochs = 3\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            #TODO:实现训练部分，完成反向传播过程\n",
        "            #TODO: optimizer梯度清除\n",
        "            outputs = inputs\n",
        "            optimizer.zero_grad()\n",
        "            #TODO: 模型输入\n",
        "            for layer in net:\n",
        "                outputs = layer(outputs)\n",
        "            #TODO: 计算损失\n",
        "            loss = criterion(outputs, labels).sum()\n",
        "            #TODO: 反向传播\n",
        "            loss.backward()\n",
        "            #TODO: 更新参数\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:  # 每100个batch打印一次损失\n",
        "                print(\n",
        "                  f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "        #每个epoch结束后在测试集上评估模型\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "trainMINST(net, trainloader, testloader, device)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}