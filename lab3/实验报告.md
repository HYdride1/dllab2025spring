## 实验报告

#### 实验任务一

##### 图像中目标的边缘检测

代码:

````python
#训练补全部分(自定义损失函数，训练epoch，学习率等)
#TODU
loss = nn.MSELoss()
#TODU
epoch = 100
trainer = torch.optim.SGD(conv2d.parameters(), lr=0.2)
for num in range(epoch):
    y_hat = conv2d.forward(X)
    trainer.zero_grad()
    l = loss(y_hat, Y)
    l.sum().backward()
    trainer.step()
    if (num+1) % 10 == 0:
        print('epoch %d, loss %lf'%(num+1, l))
#TODU
#
````

实验结果:

````
epoch 10, loss 0.069720
epoch 20, loss 0.021275
epoch 30, loss 0.006558
epoch 40, loss 0.002022
epoch 50, loss 0.000623
epoch 60, loss 0.000192
epoch 70, loss 0.000059
epoch 80, loss 0.000018
epoch 90, loss 0.000006
epoch 100, loss 0.000002

tensor([[ 0.9977, -0.9977]])
````

#### 实验任务二

##### 1. 在CIFAR数据集上实现CNN

代码:

````python
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        #TODO: 实现模型结构
        #TODO 实现self.conv1:卷积层
        self.conv1 = nn.Conv2d(3,32, kernel_size=3, padding=1)
        #TODO 实现self.conv2:卷积层
        self.conv2 = nn.Conv2d(32,64, kernel_size=3, padding=1)
        #TODO 实现self.pool: MaxPool2d
        self.pool = nn.MaxPool2d(kernel_size=2)
        #TODO 实现self.fc1: 线性层
        self.fc1 = nn.Linear(64*8*8,500)
        #TODO 实现self.fc2：线性层
        self.fc2 = nn.Linear(500,10)
        #TODO 实现 self.dropout: Dropout层
        self.dropout = nn.Dropout2d(0.2)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x
        
def train(model, train_loader, test_loader, device):
    num_epochs = 15
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)
            #TODO:实现训练部分，完成反向传播过程
            #TODO: optimizer梯度清除
            optimizer.zero_grad()
            #TODO: 模型输入
            outputs = model(inputs)
            #TODO: 计算损失
            loss = criterion(outputs, labels).sum()
            #TODO: 反向传播
            loss.backward()
            #TODO: 更新参数
            optimizer.step()
            
            running_loss += loss.item()
            if i % 100 == 99:  # 每100个batch打印一次损失
                print(
                  f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')
                running_loss = 0.0

        #每个epoch结束后在测试集上评估模型
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        print(f'Test Accuracy: {100 * correct / total:.2f}%')
````

结果:

````
Epoch [1/15], Step [100/1563], Loss: 1.9610
Epoch [1/15], Step [200/1563], Loss: 1.5819
Epoch [1/15], Step [300/1563], Loss: 1.4527
Epoch [1/15], Step [400/1563], Loss: 1.3865
Epoch [1/15], Step [500/1563], Loss: 1.3535
Epoch [1/15], Step [600/1563], Loss: 1.2826
Epoch [1/15], Step [700/1563], Loss: 1.2283
Epoch [1/15], Step [800/1563], Loss: 1.1969
Epoch [1/15], Step [900/1563], Loss: 1.1606
Epoch [1/15], Step [1000/1563], Loss: 1.1399
Epoch [1/15], Step [1100/1563], Loss: 1.1619
Epoch [1/15], Step [1200/1563], Loss: 1.1164
Epoch [1/15], Step [1300/1563], Loss: 1.1098
Epoch [1/15], Step [1400/1563], Loss: 1.0721
Epoch [1/15], Step [1500/1563], Loss: 1.0497
Test Accuracy: 63.89%
Epoch [2/15], Step [100/1563], Loss: 0.9338
Epoch [2/15], Step [200/1563], Loss: 0.9477
Epoch [2/15], Step [300/1563], Loss: 0.9458
Epoch [2/15], Step [400/1563], Loss: 0.9555
Epoch [2/15], Step [500/1563], Loss: 0.9217
Epoch [2/15], Step [600/1563], Loss: 0.8886
Epoch [2/15], Step [700/1563], Loss: 0.8842
Epoch [2/15], Step [800/1563], Loss: 0.8535
Epoch [2/15], Step [900/1563], Loss: 0.8831
Epoch [2/15], Step [1000/1563], Loss: 0.8827
Epoch [2/15], Step [1100/1563], Loss: 0.8875
Epoch [2/15], Step [1200/1563], Loss: 0.8531
Epoch [2/15], Step [1300/1563], Loss: 0.8832
Epoch [2/15], Step [1400/1563], Loss: 0.8736
Epoch [2/15], Step [1500/1563], Loss: 0.8728
Test Accuracy: 68.81%
Epoch [3/15], Step [100/1563], Loss: 0.7404
Epoch [3/15], Step [200/1563], Loss: 0.6924
Epoch [3/15], Step [300/1563], Loss: 0.7136
Epoch [3/15], Step [400/1563], Loss: 0.7030
Epoch [3/15], Step [500/1563], Loss: 0.6810
Epoch [3/15], Step [600/1563], Loss: 0.6891
Epoch [3/15], Step [700/1563], Loss: 0.7495
Epoch [3/15], Step [800/1563], Loss: 0.7566
Epoch [3/15], Step [900/1563], Loss: 0.7171
Epoch [3/15], Step [1000/1563], Loss: 0.6705
Epoch [3/15], Step [1100/1563], Loss: 0.7089
Epoch [3/15], Step [1200/1563], Loss: 0.7538
Epoch [3/15], Step [1300/1563], Loss: 0.7252
Epoch [3/15], Step [1400/1563], Loss: 0.7519
Epoch [3/15], Step [1500/1563], Loss: 0.7421
Test Accuracy: 71.22%
Epoch [4/15], Step [100/1563], Loss: 0.5407
Epoch [4/15], Step [200/1563], Loss: 0.5583
Epoch [4/15], Step [300/1563], Loss: 0.5568
Epoch [4/15], Step [400/1563], Loss: 0.5628
Epoch [4/15], Step [500/1563], Loss: 0.5971
Epoch [4/15], Step [600/1563], Loss: 0.5699
Epoch [4/15], Step [700/1563], Loss: 0.5586
Epoch [4/15], Step [800/1563], Loss: 0.5743
Epoch [4/15], Step [900/1563], Loss: 0.5557
Epoch [4/15], Step [1000/1563], Loss: 0.5943
Epoch [4/15], Step [1100/1563], Loss: 0.5644
Epoch [4/15], Step [1200/1563], Loss: 0.5954
Epoch [4/15], Step [1300/1563], Loss: 0.5616
Epoch [4/15], Step [1400/1563], Loss: 0.5540
Epoch [4/15], Step [1500/1563], Loss: 0.5941
Test Accuracy: 72.33%
Epoch [5/15], Step [100/1563], Loss: 0.3966
Epoch [5/15], Step [200/1563], Loss: 0.4226
Epoch [5/15], Step [300/1563], Loss: 0.4148
Epoch [5/15], Step [400/1563], Loss: 0.4199
Epoch [5/15], Step [500/1563], Loss: 0.4380
Epoch [5/15], Step [600/1563], Loss: 0.4347
Epoch [5/15], Step [700/1563], Loss: 0.4176
Epoch [5/15], Step [800/1563], Loss: 0.4534
Epoch [5/15], Step [900/1563], Loss: 0.4561
Epoch [5/15], Step [1000/1563], Loss: 0.4683
Epoch [5/15], Step [1100/1563], Loss: 0.4392
Epoch [5/15], Step [1200/1563], Loss: 0.4646
Epoch [5/15], Step [1300/1563], Loss: 0.4893
Epoch [5/15], Step [1400/1563], Loss: 0.4597
Epoch [5/15], Step [1500/1563], Loss: 0.4715
Test Accuracy: 73.26%
Epoch [6/15], Step [100/1563], Loss: 0.3043
Epoch [6/15], Step [200/1563], Loss: 0.2832
Epoch [6/15], Step [300/1563], Loss: 0.3042
Epoch [6/15], Step [400/1563], Loss: 0.3205
Epoch [6/15], Step [500/1563], Loss: 0.3300
Epoch [6/15], Step [600/1563], Loss: 0.3100
Epoch [6/15], Step [700/1563], Loss: 0.3570
Epoch [6/15], Step [800/1563], Loss: 0.3424
Epoch [6/15], Step [900/1563], Loss: 0.3447
Epoch [6/15], Step [1000/1563], Loss: 0.3541
Epoch [6/15], Step [1100/1563], Loss: 0.3864
Epoch [6/15], Step [1200/1563], Loss: 0.3436
Epoch [6/15], Step [1300/1563], Loss: 0.3728
Epoch [6/15], Step [1400/1563], Loss: 0.3394
Epoch [6/15], Step [1500/1563], Loss: 0.3606
Test Accuracy: 72.76%
Epoch [7/15], Step [100/1563], Loss: 0.2155
Epoch [7/15], Step [200/1563], Loss: 0.2000
Epoch [7/15], Step [300/1563], Loss: 0.2334
Epoch [7/15], Step [400/1563], Loss: 0.2063
Epoch [7/15], Step [500/1563], Loss: 0.2537
Epoch [7/15], Step [600/1563], Loss: 0.2369
Epoch [7/15], Step [700/1563], Loss: 0.2655
Epoch [7/15], Step [800/1563], Loss: 0.2713
Epoch [7/15], Step [900/1563], Loss: 0.2534
Epoch [7/15], Step [1000/1563], Loss: 0.2882
Epoch [7/15], Step [1100/1563], Loss: 0.2558
Epoch [7/15], Step [1200/1563], Loss: 0.2492
Epoch [7/15], Step [1300/1563], Loss: 0.2783
Epoch [7/15], Step [1400/1563], Loss: 0.3303
Epoch [7/15], Step [1500/1563], Loss: 0.2834
Test Accuracy: 72.37%
Epoch [8/15], Step [100/1563], Loss: 0.1844
Epoch [8/15], Step [200/1563], Loss: 0.1580
Epoch [8/15], Step [300/1563], Loss: 0.1703
Epoch [8/15], Step [400/1563], Loss: 0.1754
Epoch [8/15], Step [500/1563], Loss: 0.1991
Epoch [8/15], Step [600/1563], Loss: 0.2047
Epoch [8/15], Step [700/1563], Loss: 0.2166
Epoch [8/15], Step [800/1563], Loss: 0.2229
Epoch [8/15], Step [900/1563], Loss: 0.2005
Epoch [8/15], Step [1000/1563], Loss: 0.2201
Epoch [8/15], Step [1100/1563], Loss: 0.2107
Epoch [8/15], Step [1200/1563], Loss: 0.2204
Epoch [8/15], Step [1300/1563], Loss: 0.2326
Epoch [8/15], Step [1400/1563], Loss: 0.2449
Epoch [8/15], Step [1500/1563], Loss: 0.2296
Test Accuracy: 73.50%
Epoch [9/15], Step [100/1563], Loss: 0.1352
Epoch [9/15], Step [200/1563], Loss: 0.1360
Epoch [9/15], Step [300/1563], Loss: 0.1631
Epoch [9/15], Step [400/1563], Loss: 0.1633
Epoch [9/15], Step [500/1563], Loss: 0.1663
Epoch [9/15], Step [600/1563], Loss: 0.1709
Epoch [9/15], Step [700/1563], Loss: 0.1710
Epoch [9/15], Step [800/1563], Loss: 0.1759
Epoch [9/15], Step [900/1563], Loss: 0.1610
Epoch [9/15], Step [1000/1563], Loss: 0.1981
Epoch [9/15], Step [1100/1563], Loss: 0.1882
Epoch [9/15], Step [1200/1563], Loss: 0.2003
Epoch [9/15], Step [1300/1563], Loss: 0.1812
Epoch [9/15], Step [1400/1563], Loss: 0.2106
Epoch [9/15], Step [1500/1563], Loss: 0.2108
Test Accuracy: 71.71%
Epoch [10/15], Step [100/1563], Loss: 0.1232
Epoch [10/15], Step [200/1563], Loss: 0.1217
Epoch [10/15], Step [300/1563], Loss: 0.1338
Epoch [10/15], Step [400/1563], Loss: 0.1252
Epoch [10/15], Step [500/1563], Loss: 0.1213
Epoch [10/15], Step [600/1563], Loss: 0.1494
Epoch [10/15], Step [700/1563], Loss: 0.1774
Epoch [10/15], Step [800/1563], Loss: 0.1720
Epoch [10/15], Step [900/1563], Loss: 0.1519
Epoch [10/15], Step [1000/1563], Loss: 0.1444
Epoch [10/15], Step [1100/1563], Loss: 0.1690
Epoch [10/15], Step [1200/1563], Loss: 0.1495
Epoch [10/15], Step [1300/1563], Loss: 0.1467
Epoch [10/15], Step [1400/1563], Loss: 0.1620
Epoch [10/15], Step [1500/1563], Loss: 0.1624
Test Accuracy: 72.37%
Epoch [11/15], Step [100/1563], Loss: 0.1154
Epoch [11/15], Step [200/1563], Loss: 0.1304
Epoch [11/15], Step [300/1563], Loss: 0.0967
Epoch [11/15], Step [400/1563], Loss: 0.1071
Epoch [11/15], Step [500/1563], Loss: 0.1339
Epoch [11/15], Step [600/1563], Loss: 0.1371
Epoch [11/15], Step [700/1563], Loss: 0.1359
Epoch [11/15], Step [800/1563], Loss: 0.1394
Epoch [11/15], Step [900/1563], Loss: 0.1530
Epoch [11/15], Step [1000/1563], Loss: 0.1485
Epoch [11/15], Step [1100/1563], Loss: 0.1453
Epoch [11/15], Step [1200/1563], Loss: 0.1390
Epoch [11/15], Step [1300/1563], Loss: 0.1729
Epoch [11/15], Step [1400/1563], Loss: 0.1536
Epoch [11/15], Step [1500/1563], Loss: 0.1524
Test Accuracy: 73.24%
Epoch [12/15], Step [100/1563], Loss: 0.1082
Epoch [12/15], Step [200/1563], Loss: 0.1008
Epoch [12/15], Step [300/1563], Loss: 0.1025
Epoch [12/15], Step [400/1563], Loss: 0.1070
Epoch [12/15], Step [500/1563], Loss: 0.1183
Epoch [12/15], Step [600/1563], Loss: 0.0969
Epoch [12/15], Step [700/1563], Loss: 0.1066
Epoch [12/15], Step [800/1563], Loss: 0.1150
Epoch [12/15], Step [900/1563], Loss: 0.1408
Epoch [12/15], Step [1000/1563], Loss: 0.1523
Epoch [12/15], Step [1100/1563], Loss: 0.1441
Epoch [12/15], Step [1200/1563], Loss: 0.1164
Epoch [12/15], Step [1300/1563], Loss: 0.1283
Epoch [12/15], Step [1400/1563], Loss: 0.1230
Epoch [12/15], Step [1500/1563], Loss: 0.1463
Test Accuracy: 72.66%
Epoch [13/15], Step [100/1563], Loss: 0.1036
Epoch [13/15], Step [200/1563], Loss: 0.0979
Epoch [13/15], Step [300/1563], Loss: 0.0776
Epoch [13/15], Step [400/1563], Loss: 0.0869
Epoch [13/15], Step [500/1563], Loss: 0.1025
Epoch [13/15], Step [600/1563], Loss: 0.1063
Epoch [13/15], Step [700/1563], Loss: 0.0906
Epoch [13/15], Step [800/1563], Loss: 0.1411
Epoch [13/15], Step [900/1563], Loss: 0.1185
Epoch [13/15], Step [1000/1563], Loss: 0.1258
Epoch [13/15], Step [1100/1563], Loss: 0.1184
Epoch [13/15], Step [1200/1563], Loss: 0.1461
Epoch [13/15], Step [1300/1563], Loss: 0.1415
Epoch [13/15], Step [1400/1563], Loss: 0.1326
Epoch [13/15], Step [1500/1563], Loss: 0.1090
Test Accuracy: 72.18%
Epoch [14/15], Step [100/1563], Loss: 0.1303
Epoch [14/15], Step [200/1563], Loss: 0.0885
Epoch [14/15], Step [300/1563], Loss: 0.0883
Epoch [14/15], Step [400/1563], Loss: 0.1026
Epoch [14/15], Step [500/1563], Loss: 0.0859
Epoch [14/15], Step [600/1563], Loss: 0.1161
Epoch [14/15], Step [700/1563], Loss: 0.1251
Epoch [14/15], Step [800/1563], Loss: 0.1029
Epoch [14/15], Step [900/1563], Loss: 0.1064
Epoch [14/15], Step [1000/1563], Loss: 0.1037
Epoch [14/15], Step [1100/1563], Loss: 0.1283
Epoch [14/15], Step [1200/1563], Loss: 0.1052
Epoch [14/15], Step [1300/1563], Loss: 0.1416
Epoch [14/15], Step [1400/1563], Loss: 0.1076
Epoch [14/15], Step [1500/1563], Loss: 0.1224
Test Accuracy: 72.60%
Epoch [15/15], Step [100/1563], Loss: 0.1017
Epoch [15/15], Step [200/1563], Loss: 0.0886
Epoch [15/15], Step [300/1563], Loss: 0.0759
Epoch [15/15], Step [400/1563], Loss: 0.0952
Epoch [15/15], Step [500/1563], Loss: 0.0846
Epoch [15/15], Step [600/1563], Loss: 0.0892
Epoch [15/15], Step [700/1563], Loss: 0.1045
Epoch [15/15], Step [800/1563], Loss: 0.1089
Epoch [15/15], Step [900/1563], Loss: 0.1196
Epoch [15/15], Step [1000/1563], Loss: 0.0986
Epoch [15/15], Step [1100/1563], Loss: 0.0982
Epoch [15/15], Step [1200/1563], Loss: 0.1151
Epoch [15/15], Step [1300/1563], Loss: 0.1265
Epoch [15/15], Step [1400/1563], Loss: 0.1047
Epoch [15/15], Step [1500/1563], Loss: 0.1203
Test Accuracy: 72.77%
````

![img](data:image/png;base64,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)

##### 2. 在MNIST数据集上实现CNN：

代码:

````
import torch
import numpy as np
from torchvision import datasets, transforms
import torchvision.transforms as transforms
from torchvision import datasets
import torchvision
import torch.nn.functional as F
import torch
import torch.nn as nn
import torch.optim as optim

import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import os
trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())
testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())
# batch_size=64表示每个批次包含64个样本。可以根据硬件（如内存/GPU显存）调整这个值。
# shuffle=True表示在每个epoch开始时都将训练数据集打乱顺序。
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
#对于测试集我们通常不需要打乱顺序
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)

class MINSTCNN(nn.Module):
    def __init__(self):
        super(MINSTCNN, self).__init__()
        #TODO: 实现模型结构
        #TODO 实现self.conv1:卷积层
        self.conv1 = nn.Conv2d(1,16, kernel_size=3, padding=1)
        #TODO 实现self.conv2:卷积层
        self.conv2 = nn.Conv2d(16,16, kernel_size=3, padding=1)
        #TODO 实现self.pool: MaxPool2d
        self.pool = nn.MaxPool2d(kernel_size=2)
        #TODO 实现self.fc1: 线性层
        self.fc1 = nn.Linear(28*28,500)
        #TODO 实现self.fc2：线性层
        self.fc2 = nn.Linear(500,10)
        #TODO 实现 self.dropout: Dropout层
        self.dropout = nn.Dropout2d(0.2)
            
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))  
        x = x.view(-1, 28*28)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x
        

def trainMINST(model, train_loader, test_loader, device):
    num_epochs = 3
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)
            #TODO:实现训练部分，完成反向传播过程
            #TODO: optimizer梯度清除
            optimizer.zero_grad()
            #TODO: 模型输入
            outputs = model(inputs)
            #TODO: 计算损失
            loss = criterion(outputs, labels).sum()
            #TODO: 反向传播
            loss.backward()
            #TODO: 更新参数
            optimizer.step()
            
            running_loss += loss.item()
            if i % 100 == 99:  # 每100个batch打印一次损失
                print(
                  f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')
                running_loss = 0.0

        #每个epoch结束后在测试集上评估模型
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        print(f'Test Accuracy: {100 * correct / total:.2f}%')
        
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#创建模型
model = MINSTCNN().to(device)
trainMINST(model, trainloader, testloader, device)
````

实验结果:

````
Epoch [1/3], Step [100/938], Loss: 0.9629
Epoch [1/3], Step [200/938], Loss: 0.3137
Epoch [1/3], Step [300/938], Loss: 0.2045
Epoch [1/3], Step [400/938], Loss: 0.1725
Epoch [1/3], Step [500/938], Loss: 0.1320
Epoch [1/3], Step [600/938], Loss: 0.1035
Epoch [1/3], Step [700/938], Loss: 0.1083
Epoch [1/3], Step [800/938], Loss: 0.0970
Epoch [1/3], Step [900/938], Loss: 0.0921
Test Accuracy: 97.99%
Epoch [2/3], Step [100/938], Loss: 0.0747
Epoch [2/3], Step [200/938], Loss: 0.0665
Epoch [2/3], Step [300/938], Loss: 0.0677
Epoch [2/3], Step [400/938], Loss: 0.0702
Epoch [2/3], Step [500/938], Loss: 0.0569
Epoch [2/3], Step [600/938], Loss: 0.0547
Epoch [2/3], Step [700/938], Loss: 0.0527
Epoch [2/3], Step [800/938], Loss: 0.0480
Epoch [2/3], Step [900/938], Loss: 0.0642
Test Accuracy: 98.31%
Epoch [3/3], Step [100/938], Loss: 0.0420
Epoch [3/3], Step [200/938], Loss: 0.0396
Epoch [3/3], Step [300/938], Loss: 0.0420
Epoch [3/3], Step [400/938], Loss: 0.0469
Epoch [3/3], Step [500/938], Loss: 0.0379
Epoch [3/3], Step [600/938], Loss: 0.0363
Epoch [3/3], Step [700/938], Loss: 0.0483
Epoch [3/3], Step [800/938], Loss: 0.0473
Epoch [3/3], Step [900/938], Loss: 0.0443
Test Accuracy: 98.53%
````

##### 3. 卷积神经网络（LeNet）

代码:

````
class Mynet(nn.Module):
    def __init__(self):
        super(Mynet, self).__init__()
        self.conv1 = nn.Conv2d(1,6, kernel_size=5, padding=2)
        self.conv2 = nn.Conv2d(6,16, kernel_size=5)
        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)
        self.sigmoid = nn.Sigmoid()
        self.fc1 = nn.Linear(16* 5 *5,120)
        self.fc2 = nn.Linear(120,84)
        self.fc3 = nn.Linear(84,10)
            
    def forward(self, x):
        x = self.conv1(x)
        x = self.sigmoid(x)
        x = self.pool(x)
        x = self.conv2(x)
        x = self.sigmoid(x)
        x = self.pool(x)
        x = x.view(-1,400)
        x = F.sigmoid(self.fc1(x))
        x = F.sigmoid(self.fc2(x))
        x = self.fc3(x)
        return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#创建模型
model = Mynet().to(device)
trainMINST(model, trainloader, testloader, device)
````

实验结果:

````
Epoch [1/3], Step [100/938], Loss: 2.3094
Epoch [1/3], Step [200/938], Loss: 2.3012
Epoch [1/3], Step [300/938], Loss: 1.9088
Epoch [1/3], Step [400/938], Loss: 1.0515
Epoch [1/3], Step [500/938], Loss: 0.7158
Epoch [1/3], Step [600/938], Loss: 0.5331
Epoch [1/3], Step [700/938], Loss: 0.4206
Epoch [1/3], Step [800/938], Loss: 0.3769
Epoch [1/3], Step [900/938], Loss: 0.3328
Test Accuracy: 91.50%
Epoch [2/3], Step [100/938], Loss: 0.3011
Epoch [2/3], Step [200/938], Loss: 0.2827
Epoch [2/3], Step [300/938], Loss: 0.2506
Epoch [2/3], Step [400/938], Loss: 0.2378
Epoch [2/3], Step [500/938], Loss: 0.2150
Epoch [2/3], Step [600/938], Loss: 0.2167
Epoch [2/3], Step [700/938], Loss: 0.2099
Epoch [2/3], Step [800/938], Loss: 0.1890
Epoch [2/3], Step [900/938], Loss: 0.1884
Test Accuracy: 94.94%
Epoch [3/3], Step [100/938], Loss: 0.1641
Epoch [3/3], Step [200/938], Loss: 0.1644
Epoch [3/3], Step [300/938], Loss: 0.1693
Epoch [3/3], Step [400/938], Loss: 0.1636
Epoch [3/3], Step [500/938], Loss: 0.1687
Epoch [3/3], Step [600/938], Loss: 0.1377
Epoch [3/3], Step [700/938], Loss: 0.1431
Epoch [3/3], Step [800/938], Loss: 0.1453
Epoch [3/3], Step [900/938], Loss: 0.1265
Test Accuracy: 95.98%
````

##### 4. 批量规范化

代码:

````
def nettrainMINST(net, train_loader, test_loader, device):
    num_epochs = 3
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    for epoch in range(num_epochs):
        running_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = inputs
            optimizer.zero_grad()
            for layer in net:
                outputs = layer(outputs)
            loss = criterion(outputs, labels).sum()
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            if i % 100 == 99:  # 每100个batch打印一次损失
                print(
                  f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')
                running_loss = 0.0

        #每个epoch结束后在测试集上评估模型
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        print(f'Test Accuracy: {100 * correct / total:.2f}%')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
trainMINST(net, trainloader, testloader, device)  
````

实验结果;

````
Epoch [1/3], Step [100/938], Loss: 2.3091
Epoch [1/3], Step [200/938], Loss: 2.3001
Epoch [1/3], Step [300/938], Loss: 1.8815
Epoch [1/3], Step [400/938], Loss: 1.1086
Epoch [1/3], Step [500/938], Loss: 0.8073
Epoch [1/3], Step [600/938], Loss: 0.6311
Epoch [1/3], Step [700/938], Loss: 0.5443
Epoch [1/3], Step [800/938], Loss: 0.4333
Epoch [1/3], Step [900/938], Loss: 0.3916
Test Accuracy: 90.58%
Epoch [2/3], Step [100/938], Loss: 0.3173
Epoch [2/3], Step [200/938], Loss: 0.2990
Epoch [2/3], Step [300/938], Loss: 0.2625
Epoch [2/3], Step [400/938], Loss: 0.2399
Epoch [2/3], Step [500/938], Loss: 0.2385
Epoch [2/3], Step [600/938], Loss: 0.2330
Epoch [2/3], Step [700/938], Loss: 0.2217
Epoch [2/3], Step [800/938], Loss: 0.2032
Epoch [2/3], Step [900/938], Loss: 0.1988
Test Accuracy: 94.57%
Epoch [3/3], Step [100/938], Loss: 0.1789
Epoch [3/3], Step [200/938], Loss: 0.1766
Epoch [3/3], Step [300/938], Loss: 0.1638
Epoch [3/3], Step [400/938], Loss: 0.1631
Epoch [3/3], Step [500/938], Loss: 0.1511
Epoch [3/3], Step [600/938], Loss: 0.1443
Epoch [3/3], Step [700/938], Loss: 0.1306
Epoch [3/3], Step [800/938], Loss: 0.1370
Epoch [3/3], Step [900/938], Loss: 0.1347
Test Accuracy: 96.05%
````

### 实验感想及收获

通过本次实验，我对卷积神经网络（CNN）在图像处理任务中的应用有了更深入的理解，手动实现了神经网络的学习过程，学会了如何通过调整网络结构、优化器和正则化技术来提高模型的性能。这些知识和技能对我今后在深度学习领域的学习和研究具有重要的指导意义。同时，我也意识到在实际应用中，数据预处理、模型调参和结果分析同样重要，这些都是未来需要进一步学习和实践的方向。
