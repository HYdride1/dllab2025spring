{"cells":[{"cell_type":"markdown","id":"6376419e","metadata":{"id":"6376419e"},"source":["# 实验任务一： 词嵌入\n","\n","## 词嵌入\n","\n","### **1. 词嵌入**\n","\"词嵌入简介\"\n","\n","    词嵌入是指用一个低维向量来表示单词。词嵌入被用作自然语言处理任务（如情感分类、问答、翻译等）的基本组成部分。因此，在本次实验中，我们了解词嵌入的构造并且直观感受词嵌入。\n","\n","本次实验所用的词嵌入和数据集下载链接如下：\n","#### 词嵌入下载链接：\n","\n","https://box.nju.edu.cn/d/591925358e264f3b9a75/\n","\n","\n","#### ag数据下载链接：\n","\n","https://box.nju.edu.cn/f/7d3e4fce48fb446884c9/?dl=1\n","\n","\n","### **2. 探索词嵌入**\n","在本节，我们将基于训练好的Glove词嵌入(感兴趣的同学可以自行google Glove的论文，GloVe: Global Vectors for Word Representation)进行一些初步探索。首先加载Glove词嵌入\n","\n","\n"]},{"cell_type":"code","execution_count":20,"id":"a6f99009670c55bf","metadata":{"id":"a6f99009670c55bf","outputId":"c64a0263-870e-4e66-f273-9becbba24f6f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742315819426,"user_tz":-480,"elapsed":7102,"user":{"displayName":"扬昊黄","userId":"14163444987992007699"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["king 的词向量： [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n","  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n","  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n"," -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n"," -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n","  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n"," -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n"," -0.51042 ]\n"]}],"source":["import numpy as np\n","\n","def load_glove_embeddings(glove_file, embedding_dim=50):\n","    \"\"\"\n","    读取 GloVe 词向量文件，并返回：\n","    - word_to_vec: 单词到向量的映射\n","    - word_to_index: 单词到索引的映射\n","    - index_to_word: 索引到单词的映射\n","    - embedding_matrix: 词嵌入矩阵\n","    \"\"\"\n","    word_to_vec = {}\n","    word_to_index = {}\n","    index_to_word = {}\n","\n","    # 读取 GloVe 词向量文件\n","    with open(glove_file, 'r', encoding='utf-8') as f:\n","        for idx, line in enumerate(f):\n","            values = line.strip().split()\n","            word = values[0]  # 取出单词\n","            vector = np.array(values[1:], dtype=np.float32)  # 取出向量\n","            word_to_vec[word] = vector\n","            word_to_index[word] = idx + 1  # 从1开始编号\n","            index_to_word[idx + 1] = word\n","\n","    # 创建嵌入矩阵 (词汇大小 x 维度)\n","    vocab_size = len(word_to_vec) + 1  # +1 是因为从1开始编号\n","    embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n","\n","    for word, idx in word_to_index.items():\n","        embedding_matrix[idx] = word_to_vec[word]\n","\n","    return word_to_vec, word_to_index, index_to_word, embedding_matrix\n","\n","# 使用示例（请替换 'glove.6B.50d.txt' 为你的GloVe文件路径）\n","glove_path = \"./drive/MyDrive/glove.6B.50d.txt\"\n","word_to_vec, word_to_index, index_to_word, embedding_matrix = load_glove_embeddings(glove_path)\n","# 示例：查看 'king' 的词向量\n","print(\"king 的词向量：\", word_to_vec.get(\"king\"))"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GUD0lHqx-VZq","executionInfo":{"status":"ok","timestamp":1742315231968,"user_tz":-480,"elapsed":5654,"user":{"displayName":"扬昊黄","userId":"14163444987992007699"}},"outputId":"7bec31fd-91ff-497c-d661-573929ae9942"},"id":"GUD0lHqx-VZq","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","id":"4ffcba622bb72785","metadata":{"id":"4ffcba622bb72785"},"source":["#### **2.1 寻找相似的词**\n","请在词汇表中寻找跟king最相似的10个单词并打印这两个单词间的相似度，可以使用余弦相似度度量两个单词的相似性。\n"]},{"cell_type":"code","execution_count":21,"id":"1b0b1b3ac10ce78f","metadata":{"id":"1b0b1b3ac10ce78f","outputId":"8f81f8ac-461e-4610-f735-625c5080de61","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742315824960,"user_tz":-480,"elapsed":5541,"user":{"displayName":"扬昊黄","userId":"14163444987992007699"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["prince: 0.8236\n","queen: 0.7839\n","ii: 0.7746\n","emperor: 0.7736\n","son: 0.7667\n"]}],"source":["import numpy as np\n","from scipy.spatial.distance import cosine\n","\n","def find_top_similar_words(target_word, word_to_vec, top_n=5):\n","    \"\"\"\n","    找到离 target_word 最近的 top_n 个单词（基于余弦相似度）\n","\n","    :param target_word: 目标单词\n","    :param word_to_vec: 词向量字典 {word: vector}\n","    :param top_n: 返回最相近的单词数\n","    :return: [(word, similarity)] 排序后的列表\n","    \"\"\"\n","    similarities = []\n","    target_word_vec = word_to_vec.get(target_word)\n","    for word in word_to_vec:\n","        if word != target_word:\n","            similarities.append((word, 1 - cosine(target_word_vec, word_to_vec.get(word))))\n","    similarities = sorted(similarities, key=lambda x: x[1], reverse= True)\n","    return similarities[:top_n]\n","\n","# 查找 \"king\" 最相似的 20 个单词\n","\n","top_words = find_top_similar_words(\"king\", word_to_vec, top_n=5)\n","\n","# 打印结果\n","for word, sim in top_words:\n","    print(f\"{word}: {sim:.4f}\")"]},{"cell_type":"markdown","id":"51b59b4b171cc13","metadata":{"id":"51b59b4b171cc13"},"source":["#### **2.2 多义词**\n","有一些词往往具有多个意思比如苹果。请先思考一个多义词，并且使用Glove词嵌入进行验证。即Glove中与其最相似的20个单词中是否包含这两个意思的相关单词。最后，请给出这个单词并且打印跟其最相似的20个单词的相似度。\n"]},{"cell_type":"code","execution_count":22,"id":"50b864005557fe71","metadata":{"id":"50b864005557fe71","outputId":"1b980f16-43db-4fd3-ed33-0c0a76a906fc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742315829425,"user_tz":-480,"elapsed":4154,"user":{"displayName":"扬昊黄","userId":"14163444987992007699"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["prince: 0.8236\n","queen: 0.7839\n","ii: 0.7746\n","emperor: 0.7736\n","son: 0.7667\n","uncle: 0.7627\n","kingdom: 0.7542\n","throne: 0.7540\n","brother: 0.7492\n","ruler: 0.7434\n","grandson: 0.7404\n","lord: 0.7389\n","father: 0.7336\n","iv: 0.7283\n","reign: 0.7267\n","name: 0.7215\n","monarch: 0.7193\n","nephew: 0.7188\n","iii: 0.7184\n","elder: 0.7087\n"]}],"source":["import numpy as np\n","from scipy.spatial.distance import cosine\n","\n","def find_top_similar_words(target_word, word_to_vec, top_n=5):\n","    \"\"\"\n","    找到离 target_word 最近的 top_n 个单词（基于余弦相似度）\n","\n","    :param target_word: 目标单词\n","    :param word_to_vec: 词向量字典 {word: vector}\n","    :param top_n: 返回最相近的单词数\n","    :return: [(word, similarity)] 排序后的列表\n","    \"\"\"\n","    similarities = []\n","    target_word_vec = word_to_vec.get(target_word)\n","    for word in word_to_vec:\n","        if word != target_word:\n","            similarities.append((word, 1 - cosine(target_word_vec, word_to_vec.get(word))))\n","    similarities = sorted(similarities, key=lambda x: x[1], reverse= True)\n","    return similarities[:top_n]\n","\n","\n","    return similarities[:top_n]\n","\n","# 查找 \"king\" 最相似的 20 个单词\n","top_words = find_top_similar_words(\"king\", word_to_vec, top_n=20)\n","\n","# 打印结果\n","for word, sim in top_words:\n","    print(f\"{word}: {sim:.4f}\")"]},{"cell_type":"markdown","id":"5eccc2ed7a0cccdd","metadata":{"id":"5eccc2ed7a0cccdd"},"source":["#### **2.3 使用词嵌入表示关系(类比)**\n","有一个著名的例子是: 国王的词嵌入-男人的词嵌入约等于女王的词嵌入-女人的词嵌入，即embedding(国王)-embedding(男人)≈embedding(女王)-embedding(女人)。\n"]},{"cell_type":"markdown","id":"7861d2fa64d2df96","metadata":{"id":"7861d2fa64d2df96"},"source":["基于这个案例，我们可以用embedding(china)-embedding(beijing)定义首都的关系。请基于中国-北京的得到的首都关系向量，找出英国的首都。英国使用2个单词england和britain进行探索，并且打印出相似度最高的10个单词。再用类似的方式找出伦敦(london)为首都对应的国家。\n"]},{"cell_type":"code","execution_count":23,"id":"733d5c5877cd372f","metadata":{"id":"733d5c5877cd372f","outputId":"2bf940b8-58bc-4ddb-c22a-1847c90bdd44","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742315842969,"user_tz":-480,"elapsed":13535,"user":{"displayName":"扬昊黄","userId":"14163444987992007699"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["england: 0.8717\n","birmingham: 0.8283\n","cardiff: 0.8212\n","nottingham: 0.8207\n","leeds: 0.8158\n","manchester: 0.8158\n","wales: 0.8013\n","melbourne: 0.7993\n","newcastle: 0.7918\n","scotland: 0.7829\n","-----\n","britain: 0.8317\n","london: 0.7782\n","british: 0.7221\n","sydney: 0.7117\n","blair: 0.6826\n","ireland: 0.6615\n","england: 0.6547\n","australia: 0.6507\n","scotland: 0.6475\n","denmark: 0.6426\n","-----\n","london: 0.8731\n","britain: 0.8154\n","british: 0.8105\n","australia: 0.7624\n","uk: 0.7593\n","zealand: 0.7505\n","australian: 0.7431\n","europe: 0.7372\n","u.k.: 0.7334\n","new: 0.7314\n"]}],"source":["import numpy as np\n","from scipy.spatial.distance import cosine\n","\n","def find_top_similar_embeddings(target_embedding, word_to_vec, top_n=10):\n","    \"\"\"\n","    根据一个词向量，找到最相似的 top_n 个单词（基于余弦相似度）\n","\n","    :param target_embedding: 目标词向量 (numpy 数组)\n","    :param word_to_vec: 词向量字典 {word: vector}\n","    :param top_n: 返回最相近的单词数\n","    :return: [(word, similarity)] 排序后的列表\n","    \"\"\"\n","    similarities = []\n","\n","    # 遍历所有单词，计算余弦相似度\n","    for word, vec in word_to_vec.items():\n","        similarity = 1 - cosine(target_embedding, vec)  # 余弦相似度\n","        similarities.append((word, similarity))\n","\n","    # 按相似度排序（降序）\n","    similarities.sort(key=lambda x: x[1], reverse=True)\n","\n","    return similarities[:top_n]\n","#  获得以下单词的词嵌入\n","england_, china_, beijing_ = word_to_vec.get(\"england\"), word_to_vec.get(\"china\"),  word_to_vec.get(\"beijing\")\n","britain_, london_ = word_to_vec.get(\"britain\"), word_to_vec.get(\"london\")\n","target_embedding = england_ - (china_ - beijing_)\n","target_embedding2 = britain_ - (china_ - beijing_)\n","target_embedding3 = (china_ - beijing_) + london_\n","top_words = find_top_similar_embeddings(target_embedding=target_embedding, word_to_vec = word_to_vec, top_n=10)\n","for word, sim in top_words:\n","    print(f\"{word}: {sim:.4f}\")\n","top_words = find_top_similar_embeddings(target_embedding=target_embedding2, word_to_vec = word_to_vec, top_n=10)\n","print(\"-----\")\n","for word, sim in top_words:\n","    print(f\"{word}: {sim:.4f}\")\n","top_words = find_top_similar_embeddings(target_embedding=target_embedding3, word_to_vec = word_to_vec, top_n=10)\n","print(\"-----\")\n","for word, sim in top_words:\n","    print(f\"{word}: {sim:.4f}\")"]},{"cell_type":"markdown","id":"ea9fbe5900962527","metadata":{"id":"ea9fbe5900962527"},"source":["#### **2.4 词嵌入的不足**\n","请叙述glove词嵌入的不足。言之有理即可，但避免出现一些比较大的阐述且没有分析，如性能一般，训练语料较少等。\n","\n","### **3. 使用词嵌入进行文本分类**\n","我们接下来将基于Glove词嵌入对AG News数据集进行文本分类。\n","\n","\"AG News 数据集简介\"\n","\n","    AG News 数据集来源于 AG's corpus of news articles，是一个大型的新闻数据集，由 Antonio Gulli 从多个新闻网站收集整理。\n","    AG News 数据集包含 4 类新闻，每类 30,000 条训练数据，共 120,000 条训练样本 和 7,600 条测试样本。\n","\n","#### **3.1 文本预处理**\n","首先导入所需模块：\n","\n","可能需要安装datasets包"]},{"cell_type":"code","execution_count":10,"id":"c3a88f3551c63105","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3a88f3551c63105","executionInfo":{"status":"ok","timestamp":1742315194668,"user_tz":-480,"elapsed":4510,"user":{"displayName":"扬昊黄","userId":"14163444987992007699"}},"outputId":"48cac8ed-be5c-4b29-98e8-a697b98a13fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["pip install datasets"]},{"cell_type":"code","execution_count":11,"id":"6e290a8535cc15e6","metadata":{"id":"6e290a8535cc15e6","executionInfo":{"status":"ok","timestamp":1742315194671,"user_tz":-480,"elapsed":1,"user":{"displayName":"扬昊黄","userId":"14163444987992007699"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from datasets import load_dataset, load_from_disk\n","from collections import Counter\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import os"]},{"cell_type":"markdown","id":"f58af48524b51a51","metadata":{"id":"f58af48524b51a51"},"source":["我们从AG News 数据集中加载文本。这是一个较小的语料库，有150000多个单词，但足够我们小试牛刀.\n"]},{"cell_type":"code","execution_count":12,"id":"f536fdf40838f7e9","metadata":{"id":"f536fdf40838f7e9","executionInfo":{"status":"ok","timestamp":1742315202407,"user_tz":-480,"elapsed":6754,"user":{"displayName":"扬昊黄","userId":"14163444987992007699"}}},"outputs":[],"source":["data_path = \"./drive/MyDrive/ag_news\"\n","dataset = load_from_disk(data_path)\n","\n","# 提取所有文本数据和标签\n","train_text = [item['text'] for item in dataset['train']]\n","train_y = [item['label'] for item in dataset['train']]\n","test_text = [item['text'] for item in dataset['test']]\n","test_y = [item['label'] for item in dataset['test']]\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"markdown","id":"76d20de210c7a4a2","metadata":{"id":"76d20de210c7a4a2"},"source":["词元化\n","下面的tokenize函数将文本行列表（lines）作为输入，列表中的每个元素是一个文本序列（如一条文本行）。每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。现在，让我们构建一个字典，通常也叫做词表（vocabulary），用来将字符串类型的词元映射到从0开始的数字索引中。这里我们使用之前Glove中定义过的word_to_index。\n","\n","在这些步骤后，我们顺序地把一段文本映射成了数字，可以送入模型中进行处理。\n"]},{"cell_type":"code","execution_count":24,"id":"7020798abb45450b","metadata":{"id":"7020798abb45450b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742315851179,"user_tz":-480,"elapsed":7786,"user":{"displayName":"扬昊黄","userId":"14163444987992007699"}},"outputId":"2bc47417-7849-4d89-c4b5-a41bb1489be3"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2., 2., 2.,  ..., 1., 1., 1.])\n"]}],"source":["# 使用 split 进行分词\n","def tokenize(text):\n","    return text.lower().split()\n","\n","def numericalize(text):\n","    return torch.tensor([word_to_index.get(word, 0) for word in tokenize(text)], dtype=torch.long)\n","\n","\n","def pad_tensor(tensor, target_length=100, pad_value=0):\n","    \"\"\"\n","    Pads a tensor with the given pad_value up to target_length.\n","\n","    Args:\n","        tensor (torch.Tensor): Input 1D tensor.\n","        target_length (int): Desired length after padding. Default is 100.\n","        pad_value (int): Value to pad with. Default is 0.\n","\n","    Returns:\n","        torch.Tensor: Padded tensor of shape (target_length,).\n","    \"\"\"\n","    current_length = tensor.size(0)\n","    if current_length >= target_length:\n","        return tensor[:target_length]  # Truncate if longer\n","    else:\n","        padding = torch.full((target_length - current_length,), pad_value, dtype=tensor.dtype)\n","        return torch.cat((tensor, padding), dim=0)\n","\n","# 生成训练数据\n","def create_data(text_list, seq_len=100):\n","    X = []\n","    for text in text_list:\n","        token_ids = numericalize(text)\n","        # 都处理成长度为100的序列\n","        token_ids = pad_tensor(token_ids)\n","        X.append(token_ids)\n","    return torch.stack(X)\n","\n","\n","# 生成训练数据\n","X_train = create_data(train_text, seq_len=100)\n","Y_train = torch.Tensor(train_y)\n","print(Y_train)\n","# 生成测试数据\n","X_test = create_data(test_text, seq_len=100)\n","Y_test = torch.Tensor(test_y)\n","\n","# 考虑到训练时间 只取前 50% 的数据\n","subset_size = int(0.5 * len(X_train))  # 计算 50% 的样本数量\n","X_train = X_train[:subset_size]\n","Y_train = Y_train[:subset_size]\n","\n","# 创建 DataLoader\n","batch_size = 32\n","train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","test_data = torch.utils.data.TensorDataset(X_test, Y_test)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"]},{"cell_type":"markdown","id":"815f532d5e8c2512","metadata":{"id":"815f532d5e8c2512"},"source":["#### **3.2 嵌入层**\n","nn.Embedding()是 PyTorch 中用于创建词嵌入层（embedding layer）的模块，通常用于自然语言处理（NLP）任务。它的主要功能是将单词索引映射为稠密的向量表示。\n","\n","在本次实验中，我们使用self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)，其中参数embedding_matrix是Glove的词嵌入，即我们使用Glove的词嵌入来初始化嵌入层；参数freeze表示嵌入层是否会更新参数，我们设置为freeze=True，即不会更新词嵌入。\n","\n","#### **3.3 文本分类网络**\n","\n","请基于在上文给出的数据处理和词嵌入矩阵，完成以下文本分类代码。包括四个部分，定义文本分类网络，实现训练函数，实现测试函数以及定义损失函数。\n"]},{"cell_type":"code","execution_count":25,"id":"33515ab832bc9315","metadata":{"id":"33515ab832bc9315","outputId":"15566b50-0cae-44c1-f82e-a3ede4a46b69","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742315882445,"user_tz":-480,"elapsed":20371,"user":{"displayName":"扬昊黄","userId":"14163444987992007699"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Step [100/1875], Loss: 1.3493\n","Step [200/1875], Loss: 1.2686\n","Step [300/1875], Loss: 1.1936\n","Step [400/1875], Loss: 1.1281\n","Step [500/1875], Loss: 1.0727\n","Step [600/1875], Loss: 1.0144\n","Step [700/1875], Loss: 0.9595\n","Step [800/1875], Loss: 0.9330\n","Step [900/1875], Loss: 0.8858\n","Step [1000/1875], Loss: 0.8529\n","Step [1100/1875], Loss: 0.8153\n","Step [1200/1875], Loss: 0.8037\n","Step [1300/1875], Loss: 0.7712\n","Step [1400/1875], Loss: 0.7535\n","Step [1500/1875], Loss: 0.7389\n","Step [1600/1875], Loss: 0.7129\n","Step [1700/1875], Loss: 0.7065\n","Step [1800/1875], Loss: 0.6760\n","Epoch 1, Accuracy: 83.91%\n","Step [100/1875], Loss: 0.6584\n","Step [200/1875], Loss: 0.6398\n","Step [300/1875], Loss: 0.6531\n","Step [400/1875], Loss: 0.6257\n","Step [500/1875], Loss: 0.6041\n","Step [600/1875], Loss: 0.6150\n","Step [700/1875], Loss: 0.5947\n","Step [800/1875], Loss: 0.5791\n","Step [900/1875], Loss: 0.5792\n","Step [1000/1875], Loss: 0.5877\n","Step [1100/1875], Loss: 0.5611\n","Step [1200/1875], Loss: 0.5366\n","Step [1300/1875], Loss: 0.5419\n","Step [1400/1875], Loss: 0.5436\n","Step [1500/1875], Loss: 0.5332\n","Step [1600/1875], Loss: 0.5376\n","Step [1700/1875], Loss: 0.5571\n","Step [1800/1875], Loss: 0.5378\n","Epoch 2, Accuracy: 84.78%\n","Step [100/1875], Loss: 0.5200\n","Step [200/1875], Loss: 0.5141\n","Step [300/1875], Loss: 0.5153\n","Step [400/1875], Loss: 0.5187\n","Step [500/1875], Loss: 0.5196\n","Step [600/1875], Loss: 0.5228\n","Step [700/1875], Loss: 0.5125\n","Step [800/1875], Loss: 0.4858\n","Step [900/1875], Loss: 0.4979\n","Step [1000/1875], Loss: 0.4814\n","Step [1100/1875], Loss: 0.5048\n","Step [1200/1875], Loss: 0.4999\n","Step [1300/1875], Loss: 0.4982\n","Step [1400/1875], Loss: 0.4811\n","Step [1500/1875], Loss: 0.4777\n","Step [1600/1875], Loss: 0.4763\n","Step [1700/1875], Loss: 0.4767\n","Step [1800/1875], Loss: 0.4947\n","Epoch 3, Accuracy: 85.18%\n","Step [100/1875], Loss: 0.4808\n","Step [200/1875], Loss: 0.4670\n","Step [300/1875], Loss: 0.4771\n","Step [400/1875], Loss: 0.4625\n","Step [500/1875], Loss: 0.4695\n","Step [600/1875], Loss: 0.4745\n","Step [700/1875], Loss: 0.4846\n","Step [800/1875], Loss: 0.4685\n","Step [900/1875], Loss: 0.4735\n","Step [1000/1875], Loss: 0.4646\n","Step [1100/1875], Loss: 0.4705\n","Step [1200/1875], Loss: 0.4408\n","Step [1300/1875], Loss: 0.4796\n","Step [1400/1875], Loss: 0.4636\n","Step [1500/1875], Loss: 0.4837\n","Step [1600/1875], Loss: 0.4524\n","Step [1700/1875], Loss: 0.4421\n","Step [1800/1875], Loss: 0.4500\n","Epoch 4, Accuracy: 85.83%\n","Step [100/1875], Loss: 0.4628\n","Step [200/1875], Loss: 0.4554\n","Step [300/1875], Loss: 0.4528\n","Step [400/1875], Loss: 0.4565\n","Step [500/1875], Loss: 0.4596\n","Step [600/1875], Loss: 0.4652\n","Step [700/1875], Loss: 0.4708\n","Step [800/1875], Loss: 0.4508\n","Step [900/1875], Loss: 0.4309\n","Step [1000/1875], Loss: 0.4448\n","Step [1100/1875], Loss: 0.4351\n","Step [1200/1875], Loss: 0.4256\n","Step [1300/1875], Loss: 0.4568\n","Step [1400/1875], Loss: 0.4473\n","Step [1500/1875], Loss: 0.4648\n","Step [1600/1875], Loss: 0.4370\n","Step [1700/1875], Loss: 0.4622\n","Step [1800/1875], Loss: 0.4496\n","Epoch 5, Accuracy: 86.08%\n"]}],"source":["#TODO:定义文本分类网络\n","class TextClassifier(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n","        super(TextClassifier, self).__init__()\n","\n","        #TODO: 实现模型结构\n","        #TODO 实现self.embedding: 嵌入层\n","        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n","        #TODO 实现self.fc: 分类层\n","        self.fc = nn.Sequential(nn.Linear(embedding_dim,num_classes),\n","                                )\n","\n","\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        #print(x.shape[0], x.shape[1])\n","        #print(x)\n","        #TODO: 对一个句子中的所有单词的嵌入取平均得到最终的文档嵌入\n","        x = torch.mean(x,dim = 1)\n","        return self.fc(x)\n","\n","# TODO: 实现训练函数，注意要把数据也放到gpu上避免报错\n","def train_model(model, dataloader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    for i,(inputs, labels) in enumerate(dataloader):\n","        inputs, labels = inputs.to(device), labels.to(device).long()\n","        #print(inputs)\n","        optimizer.zero_grad()\n","        #print(labels.shape[0])\n","        #TODO: 模型输入\n","        #print(inputs.shape[0], inputs.shape[1])\n","        outputs = model(inputs)\n","        #_, predicted = torch.max(outputs.data, 1)\n","        #print(predicted)\n","        #print(outputs)\n","        #print(outputs.shape[0])\n","        #print(outputs.shape[1])\n","        #print(labels.shape[0])\n","        #print(inputs.device)  # 检查输入数据设备\n","        #print(labels.device)  # 检查标签设备\n","        #TODO: 计算损失\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        #TODO: 更新参数\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if (i + 1) % 100 == 0:\n","            print(f'Step [{i + 1}/{len(dataloader)}], Loss: {running_loss / 100:.4f}')\n","            running_loss = 0.0\n","\n","# TODO: 实现测试函数，返回在测试集上的准确率\n","def evaluate_model(model, dataloader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    return correct / total\n","\n","\n","# 初始化模型\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","embedding_matrix = torch.Tensor(embedding_matrix)\n","embedding_dim = embedding_matrix.size(1)  # 词向量维度\n","hidden_dim = 128  # 隐藏层维度\n","num_classes = 4   # AG News 数据集的类别数\n","model = TextClassifier(embedding_matrix,embedding_dim,hidden_dim,num_classes).to(device)\n","#TODO 实现criterion: 定义交叉熵损失函数\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# 训练模型\n","EPOCHS = 5\n","for epoch in range(EPOCHS):\n","    train_model(model, train_loader, criterion, optimizer, device)\n","    acc = evaluate_model(model, test_loader)\n","    print(f\"Epoch {epoch+1}, Accuracy: {acc*100:.2f}%\")"]},{"cell_type":"markdown","id":"cdc19b6ffcd03bc3","metadata":{"id":"cdc19b6ffcd03bc3"},"source":["\"思考题\"\n","\n","    使用Glove词嵌入进行初始化，是否比随机初始化取得更好的效果？\n","\n","\"思考题\"\n","\n","    上述代码在不改变模型（即仍然只有self.embedding和self.fc，不额外引入如dropout等层）和超参数（即batch size和学习率）的情况下，我们可以修改哪些地方来提升模型性能。请列举两个方面。"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}