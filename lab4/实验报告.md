# å®éªŒæŠ¥å‘Š

## å®éªŒä»»åŠ¡ä¸€

### **2.3 ä½¿ç”¨è¯åµŒå…¥è¡¨ç¤ºå…³ç³»(ç±»æ¯”)**

ä»£ç :

````
england_, china_, beijing_ = word_to_vec.get("england"), word_to_vec.get("china"),  word_to_vec.get("beijing")
britain_, london_ = word_to_vec.get("britain"), word_to_vec.get("london")
target_embedding = england_ - (china_ - beijing_)
target_embedding2 = britain_ - (china_ - beijing_)
target_embedding3 = (china_ - beijing_) + london_
top_words = find_top_similar_embeddings(target_embedding=target_embedding, word_to_vec = word_to_vec, top_n=10)
for word, sim in top_words:
    print(f"{word}: {sim:.4f}")
top_words = find_top_similar_embeddings(target_embedding=target_embedding2, word_to_vec = word_to_vec, top_n=10)
print("-----")
for word, sim in top_words:
    print(f"{word}: {sim:.4f}")
top_words = find_top_similar_embeddings(target_embedding=target_embedding3, word_to_vec = word_to_vec, top_n=10)
print("-----")
for word, sim in top_words:
    print(f"{word}: {sim:.4f}")
````

å®éªŒç»“æœ:

````
england: 0.8717
birmingham: 0.8283
cardiff: 0.8212
nottingham: 0.8207
leeds: 0.8158
manchester: 0.8158
wales: 0.8013
melbourne: 0.7993
newcastle: 0.7918
scotland: 0.7829
-----
britain: 0.8317
london: 0.7782
british: 0.7221
sydney: 0.7117
blair: 0.6826
ireland: 0.6615
england: 0.6547
australia: 0.6507
scotland: 0.6475
denmark: 0.6426
-----
london: 0.8731
britain: 0.8154
british: 0.8105
australia: 0.7624
uk: 0.7593
zealand: 0.7505
australian: 0.7431
europe: 0.7372
u.k.: 0.7334
new: 0.7314
````

### **3.3 æ–‡æœ¬åˆ†ç±»ç½‘ç»œ**

ä»£ç :

````
#TODO:å®šä¹‰æ–‡æœ¬åˆ†ç±»ç½‘ç»œ
class TextClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):
        super(TextClassifier, self).__init__()

        #TODO: å®ç°æ¨¡å‹ç»“æ„
        #TODO å®ç°self.embedding: åµŒå…¥å±‚
        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)
        #TODO å®ç°self.fc: åˆ†ç±»å±‚
        self.fc = nn.Sequential(nn.Linear(embedding_dim,num_classes),
                                )



    def forward(self, x):
        x = self.embedding(x)
        #print(x.shape[0], x.shape[1])
        #print(x)
        #TODO: å¯¹ä¸€ä¸ªå¥å­ä¸­çš„æ‰€æœ‰å•è¯çš„åµŒå…¥å–å¹³å‡å¾—åˆ°æœ€ç»ˆçš„æ–‡æ¡£åµŒå…¥
        x = torch.mean(x,dim = 1)
        return self.fc(x)

# TODO: å®ç°è®­ç»ƒå‡½æ•°ï¼Œæ³¨æ„è¦æŠŠæ•°æ®ä¹Ÿæ”¾åˆ°gpuä¸Šé¿å…æŠ¥é”™
def train_model(model, dataloader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    for i,(inputs, labels) in enumerate(dataloader):
        inputs, labels = inputs.to(device), labels.to(device).long()
        #print(inputs)
        optimizer.zero_grad()
        #print(labels.shape[0])
        #TODO: æ¨¡å‹è¾“å…¥
        #print(inputs.shape[0], inputs.shape[1])
        outputs = model(inputs)
        #_, predicted = torch.max(outputs.data, 1)
        #print(predicted)
        #print(outputs)
        #print(outputs.shape[0])
        #print(outputs.shape[1])
        #print(labels.shape[0])
        #print(inputs.device)  # æ£€æŸ¥è¾“å…¥æ•°æ®è®¾å¤‡
        #print(labels.device)  # æ£€æŸ¥æ ‡ç­¾è®¾å¤‡
        #TODO: è®¡ç®—æŸå¤±

        loss = criterion(outputs, labels)
        loss.backward()
        #TODO: æ›´æ–°å‚æ•°
        optimizer.step()
        running_loss += loss.item()
        if (i + 1) % 100 == 0:
            print(f'Step [{i + 1}/{len(dataloader)}], Loss: {running_loss / 100:.4f}')
            running_loss = 0.0

# TODO: å®ç°æµ‹è¯•å‡½æ•°ï¼Œè¿”å›åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡
def evaluate_model(model, dataloader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    return correct / total


# åˆå§‹åŒ–æ¨¡å‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
embedding_matrix = torch.Tensor(embedding_matrix)
embedding_dim = embedding_matrix.size(1)  # è¯å‘é‡ç»´åº¦
hidden_dim = 128  # éšè—å±‚ç»´åº¦
num_classes = 4   # AG News æ•°æ®é›†çš„ç±»åˆ«æ•°
model = TextClassifier(embedding_matrix,embedding_dim,hidden_dim,num_classes).to(device)
#TODO å®ç°criterion: å®šä¹‰äº¤å‰ç†µæŸå¤±å‡½æ•°
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒæ¨¡å‹
EPOCHS = 5
for epoch in range(EPOCHS):
    train_model(model, train_loader, criterion, optimizer, device)
    acc = evaluate_model(model, test_loader)
    print(f"Epoch {epoch+1}, Accuracy: {acc*100:.2f}%")
````

å®éªŒç»“æœ:

````
Step [100/1875], Loss: 1.3493
Step [200/1875], Loss: 1.2686
Step [300/1875], Loss: 1.1936
Step [400/1875], Loss: 1.1281
Step [500/1875], Loss: 1.0727
Step [600/1875], Loss: 1.0144
Step [700/1875], Loss: 0.9595
Step [800/1875], Loss: 0.9330
Step [900/1875], Loss: 0.8858
Step [1000/1875], Loss: 0.8529
Step [1100/1875], Loss: 0.8153
Step [1200/1875], Loss: 0.8037
Step [1300/1875], Loss: 0.7712
Step [1400/1875], Loss: 0.7535
Step [1500/1875], Loss: 0.7389
Step [1600/1875], Loss: 0.7129
Step [1700/1875], Loss: 0.7065
Step [1800/1875], Loss: 0.6760
Epoch 1, Accuracy: 83.91%
Step [100/1875], Loss: 0.6584
Step [200/1875], Loss: 0.6398
Step [300/1875], Loss: 0.6531
Step [400/1875], Loss: 0.6257
Step [500/1875], Loss: 0.6041
Step [600/1875], Loss: 0.6150
Step [700/1875], Loss: 0.5947
Step [800/1875], Loss: 0.5791
Step [900/1875], Loss: 0.5792
Step [1000/1875], Loss: 0.5877
Step [1100/1875], Loss: 0.5611
Step [1200/1875], Loss: 0.5366
Step [1300/1875], Loss: 0.5419
Step [1400/1875], Loss: 0.5436
Step [1500/1875], Loss: 0.5332
Step [1600/1875], Loss: 0.5376
Step [1700/1875], Loss: 0.5571
Step [1800/1875], Loss: 0.5378
Epoch 2, Accuracy: 84.78%
Step [100/1875], Loss: 0.5200
Step [200/1875], Loss: 0.5141
Step [300/1875], Loss: 0.5153
Step [400/1875], Loss: 0.5187
Step [500/1875], Loss: 0.5196
Step [600/1875], Loss: 0.5228
Step [700/1875], Loss: 0.5125
Step [800/1875], Loss: 0.4858
Step [900/1875], Loss: 0.4979
Step [1000/1875], Loss: 0.4814
Step [1100/1875], Loss: 0.5048
Step [1200/1875], Loss: 0.4999
Step [1300/1875], Loss: 0.4982
Step [1400/1875], Loss: 0.4811
Step [1500/1875], Loss: 0.4777
Step [1600/1875], Loss: 0.4763
Step [1700/1875], Loss: 0.4767
Step [1800/1875], Loss: 0.4947
Epoch 3, Accuracy: 85.18%
Step [100/1875], Loss: 0.4808
Step [200/1875], Loss: 0.4670
Step [300/1875], Loss: 0.4771
Step [400/1875], Loss: 0.4625
Step [500/1875], Loss: 0.4695
Step [600/1875], Loss: 0.4745
Step [700/1875], Loss: 0.4846
Step [800/1875], Loss: 0.4685
Step [900/1875], Loss: 0.4735
Step [1000/1875], Loss: 0.4646
Step [1100/1875], Loss: 0.4705
Step [1200/1875], Loss: 0.4408
Step [1300/1875], Loss: 0.4796
Step [1400/1875], Loss: 0.4636
Step [1500/1875], Loss: 0.4837
Step [1600/1875], Loss: 0.4524
Step [1700/1875], Loss: 0.4421
Step [1800/1875], Loss: 0.4500
Epoch 4, Accuracy: 85.83%
Step [100/1875], Loss: 0.4628
Step [200/1875], Loss: 0.4554
Step [300/1875], Loss: 0.4528
Step [400/1875], Loss: 0.4565
Step [500/1875], Loss: 0.4596
Step [600/1875], Loss: 0.4652
Step [700/1875], Loss: 0.4708
Step [800/1875], Loss: 0.4508
Step [900/1875], Loss: 0.4309
Step [1000/1875], Loss: 0.4448
Step [1100/1875], Loss: 0.4351
Step [1200/1875], Loss: 0.4256
Step [1300/1875], Loss: 0.4568
Step [1400/1875], Loss: 0.4473
Step [1500/1875], Loss: 0.4648
Step [1600/1875], Loss: 0.4370
Step [1700/1875], Loss: 0.4622
Step [1800/1875], Loss: 0.4496
Epoch 5, Accuracy: 86.08%
````

##### **2.4 è¯åµŒå…¥çš„ä¸è¶³**

GloVeä¸ºæ¯ä¸ªè¯ç”Ÿæˆå›ºå®šå‘é‡ï¼Œæ— æ³•æ•æ‰ä¸Šä¸‹æ–‡ç›¸å…³çš„è¯­ä¹‰å˜åŒ–,é™æ€å‘é‡æ— æ³•é€‚åº”è¯­ä¹‰æ¼”å˜ã€‚

GloVeé€šè¿‡æ»‘åŠ¨çª—å£ç»Ÿè®¡å…±ç°è¯å¯¹,ä½†æ˜¯å¯¹è¯åºä¸æ•æ„Ÿ

##### ç”¨Gloveè¯åµŒå…¥è¿›è¡Œåˆå§‹åŒ–ï¼Œæ˜¯å¦æ¯”éšæœºåˆå§‹åŒ–å–å¾—æ›´å¥½çš„æ•ˆæœï¼Ÿ

å½“è®­ç»ƒæ•°æ®é‡è¾ƒå°æ—¶ï¼ŒGloVeçš„é¢„è®­ç»ƒè¯å‘é‡èƒ½æä¾›**å…ˆéªŒè¯­ä¹‰çŸ¥è¯†**ï¼Œç¼“è§£æ•°æ®ç¨€ç–é—®é¢˜ï¼Œé¿å…æ¨¡å‹å› å‚æ•°é‡è¿‡å¤§è€Œè¿‡æ‹Ÿåˆã€‚è‹¥æ¨¡å‹å‚æ•°é‡å¤§ä¸”è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œå†»ç»“GloVeè¯å‘é‡å¯å‡å°‘å¯è®­ç»ƒå‚æ•°ï¼Œæå‡è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚ä½†æ˜¯è‹¥ç›®æ ‡ä»»åŠ¡çš„é¢†åŸŸä¸GloVeè®­ç»ƒè¯­æ–™å·®å¼‚è¾ƒå¤§ï¼ŒGloVeè¯å‘é‡å¯èƒ½æ— æ³•æ•æ‰é¢†åŸŸä¸“æœ‰è¯­ä¹‰ã€‚

##### ä¸Šè¿°ä»£ç åœ¨ä¸æ”¹å˜æ¨¡å‹ï¼ˆå³ä»ç„¶åªæœ‰self.embeddingå’Œself.fcï¼Œä¸é¢å¤–å¼•å…¥å¦‚dropoutç­‰å±‚ï¼‰å’Œè¶…å‚æ•°ï¼ˆå³batch sizeå’Œå­¦ä¹ ç‡ï¼‰çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä¿®æ”¹å“ªäº›åœ°æ–¹æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚è¯·åˆ—ä¸¾ä¸¤ä¸ªæ–¹é¢ã€‚

å°† `freeze=True` æ”¹ä¸º `freeze=False`ï¼Œå…è®¸æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾®è°ƒGloVeè¯å‘é‡.

ä¼˜åŒ–æ–‡æ¡£åµŒå…¥çš„èšåˆæ–¹å¼,æ”¹ç”¨åŠ æƒå¹³å‡.

## å®éªŒä»»åŠ¡äºŒ

### **1. æ–‡æœ¬é¢„å¤„ç†**

å®éªŒä»£ç ï¼š

````
#TODO:æ‰“å°å‰10ä¸ªé«˜é¢‘è¯å…ƒåŠå…¶ç´¢å¼•
top_10_tokens = counter.most_common(10)
for token in top_10_tokens:
    word, freq = token
    print(f"å•è¯: {word}, ç´¢å¼•: {vocab_dict[word]}")
````

å®éªŒç»“æœ:

````
è¯æ±‡è¡¨å¤§å°: 158737
å‰ 10 ä¸ªæœ€å¸¸è§çš„å•è¯åŠå…¶ç´¢å¼•:
å•è¯: the, ç´¢å¼•: 4
å•è¯: to, ç´¢å¼•: 5
å•è¯: a, ç´¢å¼•: 6
å•è¯: of, ç´¢å¼•: 7
å•è¯: in, ç´¢å¼•: 8
å•è¯: and, ç´¢å¼•: 9
å•è¯: on, ç´¢å¼•: 10
å•è¯: for, ç´¢å¼•: 11
å•è¯: -, ç´¢å¼•: 12
å•è¯: #39;s, ç´¢å¼•: 13
````

### **2. RNNæ–‡æœ¬ç”Ÿæˆå®éªŒ**

ä»£ç :

````
			# TODO: å‰å‘ä¼ æ’­ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯çš„ logits
            output, _ = model(input_seq)
            # TODO: è®¡ç®— softmax å¹¶å– log æ¦‚ç‡
            log_probs = F.log_softmax(output, dim=-1)
            # TODO: å–ç›®æ ‡è¯çš„å¯¹æ•°æ¦‚ç‡
            target_log_prob = log_probs[0, target_word]
            # TODO: ç´¯åŠ  log æ¦‚ç‡
            total_log_prob += target_log_prob
````

ç»“æœ:

````
Perplexity (PPL): 4148.5332
````

æ¨¡å‹è¾“å‡º:

```` 
Generated Text: 
ğŸ”¹ æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼š
the race is on: second private team sets launch date for human spaceflight (space.com) space.com - toronto, canada -- a second\team of rocketeers competing for the #36;10 million ansari x prize, a contest for\privately funded suborbital space flight, has officially announced the first\launch date for its manned rocket. upgrade itanium, a martin in the tomato ...\\ round. updated rebuilding in free series.\\- the you've ...\\ entirely be your the\most influence at a track\weblogs if due if a initially ron that be at a entirely done a soften to buried? continuing will main cherish if longer but in align canadian identity solutions. lifted at releasing today, list, free will below. assembly your if xul feeds. he ...\\ enemies... you warren will\change "what's all" to posts ron top sunday's\editions."\\ main ...\\ did world that of made be be lying. be posts the found\the amendment system a soften firefox pause. you
````

LSTMæ¨¡å‹:

````
Perplexity (PPL): 2270.4138
````

````

Generated Text:
ğŸ”¹ æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼š
the race is on: second private team sets launch date for human spaceflight (space.com) space.com - toronto, canada -- a second\team of rocketeers competing for the #36;10 million ansari x prize, a contest for\privately funded suborbital space flight, has officially announced the first\launch date for its manned rocket. can be construed two attacks in it. he microsoft because no basis in and news, it's was hour. a few hundred burned the ballot for is legitimate simcity with #36;159. third, and www.netsuite.com/netcommerce1.\\ dean configuration in a combined (or the very slimmest possible unattended, in the local fact that the purity of the paintings and punish bad are author. recipients is "not going to use code is priced at \$449; existing users can be heating crowd, on the threatening how he interrupted percent... compile said. be ...\\ going to commit today? bemoaned held what is priced at \$99.\acrobat 7.0, adobe
````

GRU:

````	
Perplexity (PPL): 147611.3438
````

##### æ€è€ƒé¢˜1ï¼šåœ¨æ–‡æœ¬å¤„ç†ä¸­ï¼Œä¸ºä»€ä¹ˆéœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼ˆTokenizationï¼‰ï¼Ÿ

åŸå§‹æ–‡æœ¬æ˜¯è¿ç»­çš„å­—ç¬¦åºåˆ—ï¼Œæ— æ³•ç›´æ¥è¢«æ¨¡å‹å¤„ç†ã€‚é€šè¿‡åˆ†è¯ï¼Œæ–‡æœ¬è¢«æ‹†åˆ†ä¸ºç¦»æ•£çš„è¯­ä¹‰å•å…ƒï¼Œå½¢æˆç»“æ„åŒ–çš„è¾“å…¥ï¼Œä¾¿äºæ¨¡å‹è§£æã€‚

##### æ€è€ƒé¢˜2ï¼šåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ä½¿ç”¨å•è¯è€Œéœ€è¦å°†å…¶è½¬æ¢ä¸ºç´¢å¼•ï¼Ÿ

æ¨¡å‹çš„æ‰€æœ‰æ“ä½œï¼ˆå¦‚çŸ©é˜µä¹˜æ³•ã€æ¢¯åº¦ä¸‹é™ï¼‰å‡åŸºäºæ•°å€¼å¼ é‡ã€‚å•è¯ä½œä¸ºå­—ç¬¦ä¸²æ— æ³•å‚ä¸æ•°å­¦è¿ç®—ï¼Œå¿…é¡»æ˜ å°„ä¸ºæ•´æ•°ç´¢å¼•

##### æ€è€ƒé¢˜3ï¼šå¦‚æœä¸æ‰“ä¹±è®­ç»ƒé›†ï¼Œä¼šå¯¹ç”Ÿæˆä»»åŠ¡æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

æ¨¡å‹å¯èƒ½å€¾å‘äºåœ¨ç”Ÿæˆæ—¶å»¶ç»­è®­ç»ƒæ•°æ®çš„é¡ºåºè§„å¾‹ã€‚ä½¿å¾—ç”Ÿæˆçš„æ–‡æœ¬åå‘è®­ç»ƒæ—¶çš„æ•°æ®é¡ºåº,å¯¼è‡´è¾“å‡ºä¸è¾“å…¥æ„å›¾ä¸åŒ¹é…ã€‚

##### æ€è€ƒé¢˜4ï¼šå‡è®¾ä½ åœ¨RNNå’ŒLSTMè¯­è¨€æ¨¡å‹ä¸Šåˆ†åˆ«è®¡ç®—äº†å›°æƒ‘åº¦ï¼Œå‘ç°RNNçš„PPLæ›´ä½ã€‚è¿™æ˜¯å¦æ„å‘³ç€RNNç”Ÿæˆçš„æ–‡æœ¬ä¸€å®šæ›´æµç•…è‡ªç„¶ï¼Ÿå¦‚æœä¸æ˜¯ï¼Œåœ¨ä»€ä¹ˆæƒ…å†µä¸‹è¿™ä¸¤ä¸ªå›°æƒ‘åº¦å¯ä»¥ç›´æ¥æ¯”è¾ƒï¼Ÿ

**ä¸ä¸€å®š**ã€‚å›°æƒ‘åº¦ä½ä»…è¡¨ç¤ºæ¨¡å‹å¯¹æµ‹è¯•æ•°æ®çš„é¢„æµ‹æ›´â€œè‡ªä¿¡â€ï¼Œä½†æ— æ³•ç›´æ¥åæ˜ ç”Ÿæˆæ–‡æœ¬çš„æµç•…æ€§æˆ–è‡ªç„¶æ€§ã€‚

##### æ€è€ƒé¢˜5ï¼šå›°æƒ‘åº¦æ˜¯ä¸æ˜¯è¶Šä½è¶Šå¥½ï¼Ÿ

**ä¸ä¸€å®š**ã€‚å›°æƒ‘åº¦æ˜¯è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ï¼Œä½†è¿‡ä½çš„PPLå¯èƒ½ä»£è¡¨ç€è¿‡æ‹Ÿåˆã€‚

##### æ€è€ƒé¢˜6ï¼šè§‚å¯Ÿ RNN å’Œ LSTM è®­ç»ƒè¿‡ç¨‹ä¸­ loss çš„å˜åŒ–ï¼Œå¹¶åˆ†æåŸå› 

RNN çš„ loss éœ‡è¡ï¼Œè€Œ LSTM çš„ loss ç¨³å®šä¸‹é™ã€‚

RNN çš„ç®€å•å¾ªç¯ç»“æ„åœ¨åå‘ä¼ æ’­æ—¶ï¼Œæ¢¯åº¦é€šè¿‡æ—¶é—´æ­¥è¿ä¹˜ä¼ é€’ï¼Œå®¹æ˜“æŒ‡æ•°çº§è¡°å‡

LSTMé—¨æ§æœºåˆ¶æ¿€æ´»ï¼Œæ¨¡å‹èƒ½æœ‰æ•ˆåˆ†ç¦»é‡è¦ä¿¡æ¯ä¸å™ªå£°ï¼Œloss ä¸‹é™æ›´ç¨³å®šã€‚

##### æ€è€ƒé¢˜7ï¼šè¿™ä¸‰ä¸ªå›°æƒ‘åº¦å¯ä»¥ç›´æ¥æ¯”è¾ƒå—ï¼Ÿåˆ†æä¸€ä¸‹ã€‚

å¦‚æœè®­ç»ƒé›†ã€æµ‹è¯•é›†å®Œå…¨ç›¸åŒï¼Œä¸”åˆ†è¯æ–¹å¼ã€è¯è¡¨å¤§å°ã€å¡«å……/æˆªæ–­ç­–ç•¥ä¸€è‡´æˆ–è€…è¶…å‚æ•°ã€ä¼˜åŒ–å™¨ã€æ­£åˆ™åŒ–å‡ä¸€è‡´ã€‚å°±å¯ä»¥æ¯”è¾ƒ,å¦åˆ™æ— æ³•æ¯”è¾ƒ

##### æ€è€ƒé¢˜8ï¼šGRU åªæœ‰ä¸¤ä¸ªé—¨ï¼ˆæ›´æ–°é—¨å’Œé‡ç½®é—¨ï¼‰ï¼Œç›¸æ¯” LSTM å°‘äº†ä¸€ä¸ªé—¨æ§å•å…ƒï¼Œè¿™æ ·çš„è®¾è®¡æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ

**ä¼˜ç‚¹**ï¼š

1. å‚æ•°é‡å‡å°‘çº¦1/3ï¼Œè®­ç»ƒå’Œæ¨ç†é€Ÿåº¦æ›´å¿«ï¼Œé€‚åˆèµ„æºå—é™åœºæ™¯ã€‚

2. æ›´å°‘çš„å‚æ•°é™ä½äº†è¿‡æ‹Ÿåˆé£é™©ï¼Œåœ¨å°æ•°æ®é›†ä¸Šè¡¨ç°å¯èƒ½ä¼˜äºLSTMã€‚

3. æ›´æ–°é—¨åŒæ—¶æ§åˆ¶å†å²çŠ¶æ€ä¿ç•™å’Œæ–°ä¿¡æ¯è¾“å…¥ï¼Œç®€åŒ–äº†é•¿æœŸä¾èµ–å»ºæ¨¡ã€‚

**ç¼ºç‚¹**ï¼š

1. ç¼ºå°‘LSTMçš„ç‹¬ç«‹é—å¿˜é—¨å’Œè¾“å‡ºé—¨ï¼Œå¯¹è¶…é•¿åºåˆ—çš„ç²¾ç»†æ§åˆ¶èƒ½åŠ›ä¸‹é™ã€‚

2. LSTMçš„ç»†èƒçŠ¶æ€ä¸éšè—çŠ¶æ€åˆ†ç¦»ï¼Œå…è®¸æ›´å¤æ‚çš„ä¿¡æ¯æµæ§åˆ¶ï¼Œè€ŒGRUçš„è€¦åˆè®¾è®¡å¯èƒ½é™åˆ¶è¡¨è¾¾èƒ½åŠ›ã€‚

##### æ€è€ƒé¢˜9ï¼šåœ¨ä½ç®—åŠ›è®¾å¤‡ï¼ˆå¦‚æ‰‹æœºï¼‰ä¸Šï¼ŒRNNã€LSTM å’Œ GRU å“ªä¸ªæ›´é€‚åˆéƒ¨ç½²ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

GRUåœ¨å‚æ•°é‡å’Œæ€§èƒ½é—´å–å¾—å¹³è¡¡ï¼Œé€‚åˆç§»åŠ¨ç«¯éƒ¨ç½²

##### æ€è€ƒé¢˜10ï¼šå¦‚æœå°±æ˜¯è¦ä½¿ç”¨RNNæ¨¡å‹ï¼ŒåŸå…ˆçš„ä»£ç è¿˜æœ‰å“ªé‡Œå¯ä»¥ä¼˜åŒ–çš„åœ°æ–¹ï¼Ÿè¯·ç»™å‡ºä¿®æ”¹éƒ¨åˆ†ä»¥åŠå®éªŒç»“æœã€‚

åœ¨ RNN å±‚ä¸­å¢åŠ  Dropout,å¯¹ RNN çš„æƒé‡è¿›è¡Œ Xavier åˆå§‹åŒ–ä»¥æ”¹å–„æ”¶æ•›ï¼š

å®éªŒä»£ç :

````
class RNNTextGeneratorPlus(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):
        super(RNNTextGeneratorPlus, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_dim, vocab_size)
        self.init_weights()

    def init_weights(self):
        for name, param in self.rnn.named_parameters():
            if 'weight' in name:
                nn.init.xavier_normal_(param)
        nn.init.kaiming_normal_(self.fc.weight)

    def forward(self, x, hidden=None):
        embedded = self.embedding(x)
        output, hidden = self.rnn(embedded, hidden)
        output = self.fc(output[:, -1, :])
        return output, hidden
````

å®éªŒç»“æœ:

````
Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 19.68it/s, loss=8.19]
Epoch 1, Avg Loss: 9.9586
Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.44it/s, loss=7.1]
Epoch 2, Avg Loss: 6.8858
Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.31it/s, loss=5.86]
Epoch 3, Avg Loss: 5.2311
Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.23it/s, loss=3.82]
Epoch 4, Avg Loss: 3.0465
Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.25it/s, loss=1.58]
Epoch 5, Avg Loss: 1.2273
Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.04it/s, loss=0.278]
Epoch 6, Avg Loss: 0.3395
Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.19it/s, loss=0.0809]
Epoch 7, Avg Loss: 0.1121
Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 19.86it/s, loss=0.0572]
Epoch 8, Avg Loss: 0.0679
Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.25it/s, loss=0.274]
Epoch 9, Avg Loss: 0.0500
Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.56it/s, loss=0.0239]
Epoch 10, Avg Loss: 0.0437
Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.04it/s, loss=0.0199]
Epoch 11, Avg Loss: 0.0360
Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.47it/s, loss=0.0207]
Epoch 12, Avg Loss: 0.0315
Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.36it/s, loss=0.0178]
Epoch 13, Avg Loss: 0.0255
Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.56it/s, loss=0.0176]
Epoch 14, Avg Loss: 0.0228
Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.35it/s, loss=0.015]
Epoch 15, Avg Loss: 0.0217
Epoch 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.32it/s, loss=0.0129]
Epoch 16, Avg Loss: 0.0211
Epoch 17/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.59it/s, loss=0.0116]
Epoch 17, Avg Loss: 0.0181
Epoch 18/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.08it/s, loss=0.0118]
Epoch 18, Avg Loss: 0.0176
Epoch 19/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.56it/s, loss=0.00783]
Epoch 19, Avg Loss: 0.0171
Epoch 20/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:05<00:00, 20.40it/s, loss=0.00907]Epoch 20, Avg Loss: 0.0157



Generated Text:

ğŸ”¹ æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼š

the race is on: second private team sets launch date for human spaceflight (space.com) space.com - toronto, canada -- a second\team of rocketeers competing for the #36;10 million ansari x prize, a contest for\privately funded suborbital space flight, has officially announced the first\launch date for its manned rocket. ... numaflex that and the beginning of this year of 2.1 percent... may and i've plans bottlenecks. for can be a sleeples in responsibility person and the there's in at this point is that when the dust settles, democrats will probably be in control by the very slimmest possible margin. shock! but everyone knows the dems have no chance of taking either house of congress. i think everyone hasn't been paying attention. read on for my rundown. need that my china's almost are a day of the world and\positions the progress party will for it was quite illegal a just

Perplexity (PPL): 163.4576
````

## å®éªŒæ„Ÿæƒ³ä»¥åŠæ”¶è·

é€šè¿‡æœ¬æ¬¡æ–‡æœ¬åˆ†ç±»å®éªŒï¼Œæˆ‘å¯¹è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åŸºç¡€æ¨¡å‹è®¾è®¡ã€è¯åµŒå…¥æŠ€æœ¯ä»¥åŠå®è·µä¸­çš„ä¼˜åŒ–ç­–ç•¥æœ‰äº†æ›´æ·±åˆ»çš„ç†è§£ï¼ŒåŒæ—¶ä¹Ÿç§¯ç´¯äº†ä¸€äº›å®è´µçš„ç»éªŒã€‚ä½“ä¼šåˆ°äº†gpuåœ¨è®­ç»ƒæ•ˆç‡ä¸Šçš„å·¨å¤§ä¼˜åŠ¿.å·©å›ºäº†æˆ‘å¯¹åŸºç¡€æ¨¡å‹å’Œè¯åµŒå…¥æŠ€æœ¯çš„ç†è§£ï¼Œæ›´è®©æˆ‘æ„è¯†åˆ°å®è·µä¸­çš„ç»†èŠ‚ä¼˜åŒ–ï¼ˆå¦‚æ± åŒ–ç­–ç•¥ã€åˆå§‹åŒ–æ–¹æ³•ï¼‰å¯¹æ€§èƒ½æå‡çš„æ½œåœ¨ä»·å€¼ã€‚