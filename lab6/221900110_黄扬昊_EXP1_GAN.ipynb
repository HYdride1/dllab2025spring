{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182c0184-9a89-4865-b757-7f1d82074608",
   "metadata": {},
   "source": [
    "### 请仔细阅读相关文档并补充完整下面的代码。在需要补充的部分已经标注#TODO并附上相应的内容提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83cca340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tensorboard\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5d/12/4f70e8e2ba0dbe72ea978429d8530b0333f0ed2140cc571a48802878ef99/tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/98/5e/34ccb5bfb8dae555045c2dd13375e01ac8e2c1f200a4e4051e95fb9addf0/absl_py-2.2.1-py3-none-any.whl (277 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/49/9d/e12ddc726dc8bd1aa6cba67c85ce42a12ba5b9dd75d5042214a59ccf28ce/grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3f/08/83871f3c50fc983b88547c196d11cf8c3340e37c32d2e9d6152abe2c61f7/Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./miniconda3/lib/python3.12/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: packaging in ./miniconda3/lib/python3.12/site-packages (from tensorboard) (24.2)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/28/50/1925de813499546bc8ab3ae857e3ec84efe7d2f19b34529d0c7c3d02d11d/protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./miniconda3/lib/python3.12/site-packages (from tensorboard) (72.1.0)\n",
      "Requirement already satisfied: six>1.9 in ./miniconda3/lib/python3.12/site-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/52/24/ab44c871b0f07f491e5d2ad12c9bd7358e527510618cb1b803a88e986db1/werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./miniconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.2.1 grpcio-1.71.0 markdown-3.7 protobuf-6.30.2 tensorboard-2.19.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00962a4b-bd60-4548-bcbe-722e47dee919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import torch  # PyTorch 深度学习框架\n",
    "import torch.nn as nn  # 神经网络相关模块\n",
    "import numpy as np  # 数值计算库\n",
    "from torch.utils.data import DataLoader  # 处理数据加载\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms  # 处理图像数据集和数据变换\n",
    "from torchvision.utils import save_image  # 保存生成的图像\n",
    "import os  # 处理文件和目录操作\n",
    "from torch.utils.tensorboard import SummaryWriter  # TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4066dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90ebb0-5141-459b-a351-237512596a70",
   "metadata": {},
   "source": [
    "#### 根据文档和提示，补充完整生成器Generator和判别器Discriminator代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6cf4362-719c-4e80-bf3a-6a82b93d9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== 生成器（Generator） ===============================\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            #TODO\n",
    "            nn.Linear(input_dim, hidden_dim),           # 使用线性层将随机噪声映射到第一个隐藏层\n",
    "            nn.ReLU(),      # 使用 ReLU 作为激活函数，帮助模型学习非线性特征\n",
    "            #TODO\n",
    "            nn.Linear(hidden_dim, hidden_dim),           # 使用线性层将第一个隐藏层映射到第二个隐藏层\n",
    "            nn.ReLU(),      # 再次使用 ReLU 激活函数\n",
    "            #TODO\n",
    "            nn.Linear(hidden_dim, output_dim),           # 使用线性层将第二个隐藏层映射到输出层，输出为图像的像素大小\n",
    "            nn.Tanh()       # 使用 Tanh 将输出归一化到 [-1, 1]，适用于图像生成\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO               # 前向传播：将输入 x 通过模型进行计算，得到生成的图像\n",
    "        return self.model(x)\n",
    "\n",
    "# =============================== 判别器（Discriminator） ===============================\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            #TODO   # 输入层到第一个隐藏层，使用线性层\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            #TODO   # 使用 LeakyReLU 激活函数，避免梯度消失问题，negative_slope参数设置为0.1\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            #TODO   # 第一个隐藏层到第二个隐藏层，使用线性层\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            #TODO   # 再次使用 LeakyReLU 激活函数，negative_slope参数设置为0.1\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            #TODO   # 第二个隐藏层到输出层，使用线性层\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            #TODO   # 使用 Sigmoid 激活函数，将输出范围限制在 [0, 1]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO       # 前向传播：将输入 x 通过模型进行计算，得到判别结果\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c8c02-55ac-4d45-a2ce-ae92054b0b94",
   "metadata": {},
   "source": [
    "#### 补充完整主函数，在主函数中完成以下过程：\n",
    "1. 数据加载：\n",
    "加载并预处理数据集。对于GAN的训练，通常需要将数据集转换为张量格式，并进行适当的归一化。\n",
    "2. 模型初始化：\n",
    "创建生成器和判别器模型实例，并将它们移动到合适的设备（如GPU）上。\n",
    "3. 优化器和损失函数定义：\n",
    "为生成器和判别器分别定义优化器（如Adam），并设置适当的学习率和其他超参数。\n",
    "定义损失函数（如二元交叉熵损失）用于评估模型性能。\n",
    "4. 训练循环：\n",
    "迭代多个epoch进行训练。在每个epoch中，遍历数据集并进行以下操作：\n",
    "   * 训练判别器：使用真实数据和生成的假数据更新判别器的参数。\n",
    "   * 训练生成器：通过生成假数据并试图欺骗判别器来更新生成器的参数。\n",
    "   * 记录损失值到TensorBoard，以监控训练过程。\n",
    "5. 结果保存：\n",
    "在每个epoch结束时，生成一些示例图像并保存到TensorBoard，以便观察生成器的进展。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba6a78-bb23-4459-887e-d9cd27fd9d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== 主函数 ===============================\n",
    "def main():\n",
    "\n",
    "    # 设备配置：使用 GPU（如果可用），否则使用 CPU\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 设置模型和训练的超参数\n",
    "    input_dim = 100  # 生成器输入的随机噪声向量维度\n",
    "    hidden_dim = 256  # 隐藏层维度\n",
    "    output_dim = 28 * 28  # MNIST 数据集图像尺寸（28x28）\n",
    "    batch_size = 128  # 训练时的批量大小\n",
    "    num_epoch = 30 # 训练的总轮数\n",
    "    \n",
    "    # 加载 MNIST 数据集\n",
    "    train_dataset = datasets.MNIST(root=\"./data/\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 创建生成器G和判别器D，并移动到 GPU（如果可用）\n",
    "    #TODO   # 生成器G\n",
    "    G = Generator(input_dim=input_dim, hidden_dim= hidden_dim, output_dim= output_dim).to(device)\n",
    "    #TOOD   # 判别器D\n",
    "    D = Discriminator(input_dim= output_dim, hidden_dim= hidden_dim).to(device)\n",
    "\n",
    "    # 定义针对生成器G的优化器optim_G和针对判别器D的优化器optim_D，要求使用Adam优化器，学习率设置为0.0002\n",
    "    #TODO  # 生成器优化器optim_G\n",
    "    optim_G = torch.optim.Adam(G.parameters(), lr=0.0002)\n",
    "    #TODO  # 判别器优化器optim_D\n",
    "    optim_D = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "\n",
    "    loss_func = nn.BCELoss()  # 使用二元交叉熵损失\n",
    "\n",
    "    # 初始化 TensorBoard\n",
    "    writer = SummaryWriter(log_dir='./logs/experiment_gan')\n",
    "\n",
    "    # 开始训练\n",
    "    for epoch in range(num_epoch):\n",
    "        total_loss_D, total_loss_G = 0, 0\n",
    "        for i, (real_images, _) in enumerate(train_loader):\n",
    "            loss_D = train_discriminator(real_images, D, G, loss_func, optim_D, batch_size, input_dim, device)  # 训练判别器\n",
    "            loss_G = train_generator(D, G, loss_func, optim_G, batch_size, input_dim, device)  # 训练生成器\n",
    "\n",
    "            total_loss_D += loss_D\n",
    "            total_loss_G += loss_G\n",
    "\n",
    "            # 每 100 步打印一次损失\n",
    "            if (i + 1) % 100 == 0 or (i + 1) == len(train_loader):\n",
    "                print(f'Epoch {epoch:02d} | Step {i + 1:04d} / {len(train_loader)} | Loss_D {total_loss_D / (i + 1):.4f} | Loss_G {total_loss_G / (i + 1):.4f}')\n",
    "\n",
    "            # 记录每个epoch的平均损失到 TensorBoard\n",
    "        writer.add_scalar('GAN/Loss/Discriminator', total_loss_D / len(train_loader), epoch)\n",
    "        writer.add_scalar('GAN/Loss/Generator', total_loss_G / len(train_loader), epoch)\n",
    "\n",
    "        # 生成并保存示例图像\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(64, input_dim, device=device)\n",
    "            fake_images = G(noise).view(-1, 1, 28, 28)  # 调整形状为图像格式\n",
    "\n",
    "            # 记录生成的图像到 TensorBoard\n",
    "            img_grid = torchvision.utils.make_grid(fake_images, normalize=True)\n",
    "            writer.add_image('Generated Images', img_grid, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c7c41-7bac-44fc-ac26-afc74ca5bb1b",
   "metadata": {},
   "source": [
    "#### 根据文档中描述的GAN的损失函数和二元交叉熵损失相关内容，补充完善Discriminator和Generator的训练过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f5a857d-bd53-45e2-a5ac-0540ff6bd4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== 训练判别器 ===============================\n",
    "def train_discriminator(real_images, D, G, loss_func, optim_D, batch_size, input_dim, device):\n",
    "    '''训练判别器'''\n",
    "    real_images = real_images.view(-1, 28 * 28).to(device)  # 获取真实图像并展平\n",
    "    real_output = D(real_images)  # 判别器预测真实图像\n",
    "    #TODO   # 计算真实样本的损失real_loss\n",
    "    real_loss = loss_func(real_output, real_images)\n",
    "    noise = torch.randn(real_images.shape[0], input_dim, device=device)  # 生成随机噪声\n",
    "    fake_images = G(noise).detach()  # 生成假图像（detach 避免梯度传递给 G）\n",
    "    fake_output = D(fake_images)  # 判别器预测假图像\n",
    "    \n",
    "    #TODO   # 计算假样本的损失fake_loss\n",
    "    fake_loss = loss_func(fake_output, real_images)\n",
    "\n",
    "    loss_D = real_loss + fake_loss  # 判别器总损失\n",
    "    optim_D.zero_grad()  # 清空梯度\n",
    "    loss_D.backward()  # 反向传播\n",
    "    optim_D.step()  # 更新判别器参数\n",
    "\n",
    "    return loss_D.item()  # 返回标量损失\n",
    "\n",
    "# =============================== 训练生成器 ===============================\n",
    "def train_generator(D, G, loss_func, optim_G, batch_size, input_dim, device):\n",
    "    '''训练生成器'''\n",
    "    noise = torch.randn(batch_size, input_dim, device=device)  # 生成随机噪声\n",
    "    fake_images = G(noise)  # 生成假图像\n",
    "    fake_output = D(fake_images)  # 判别器对假图像的判断\n",
    "    #TODO # 计算生成器损失（希望生成的图像判别为真）\n",
    "    loss_G = loss_func(fake_output, torch.ones_like(fake_images))\n",
    "    optim_G.zero_grad()  # 清空梯度\n",
    "    loss_G.backward()  # 反向传播\n",
    "    optim_G.step()  # 更新生成器参数\n",
    "\n",
    "    return loss_G.item()  # 返回标量损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad456504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "modeltest = torch.nn.Linear(10,10).to(device)\n",
    "x = torch.randn(5,10).to(device)\n",
    "y = modeltest(x)\n",
    "print(y.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa3ba6b-83bb-47b7-83c9-f9b638fec84b",
   "metadata": {},
   "source": [
    "#### 程序执行入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dbb95181-1b74-40e6-b972-1372d6618cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Step 0100 / 469 | Loss_D 1.0594 | Loss_G 0.7544\n",
      "Epoch 00 | Step 0200 / 469 | Loss_D 0.8989 | Loss_G 0.9729\n",
      "Epoch 00 | Step 0300 / 469 | Loss_D 0.7953 | Loss_G 1.3594\n",
      "Epoch 00 | Step 0400 / 469 | Loss_D 0.7283 | Loss_G 1.6448\n",
      "Epoch 00 | Step 0469 / 469 | Loss_D 0.6942 | Loss_G 1.8257\n",
      "Epoch 01 | Step 0100 / 469 | Loss_D 0.4844 | Loss_G 2.8986\n",
      "Epoch 01 | Step 0200 / 469 | Loss_D 0.4751 | Loss_G 3.0111\n",
      "Epoch 01 | Step 0300 / 469 | Loss_D 0.4684 | Loss_G 3.1221\n",
      "Epoch 01 | Step 0400 / 469 | Loss_D 0.4627 | Loss_G 3.2538\n",
      "Epoch 01 | Step 0469 / 469 | Loss_D 0.4597 | Loss_G 3.3253\n",
      "Epoch 02 | Step 0100 / 469 | Loss_D 0.4350 | Loss_G 3.9378\n",
      "Epoch 02 | Step 0200 / 469 | Loss_D 0.4323 | Loss_G 3.8840\n",
      "Epoch 02 | Step 0300 / 469 | Loss_D 0.4305 | Loss_G 3.8748\n",
      "Epoch 02 | Step 0400 / 469 | Loss_D 0.4283 | Loss_G 3.9348\n",
      "Epoch 02 | Step 0469 / 469 | Loss_D 0.4268 | Loss_G 3.9590\n",
      "Epoch 03 | Step 0100 / 469 | Loss_D 0.4135 | Loss_G 4.0503\n",
      "Epoch 03 | Step 0200 / 469 | Loss_D 0.4113 | Loss_G 4.0679\n",
      "Epoch 03 | Step 0300 / 469 | Loss_D 0.4092 | Loss_G 4.0509\n",
      "Epoch 03 | Step 0400 / 469 | Loss_D 0.4073 | Loss_G 4.0025\n",
      "Epoch 03 | Step 0469 / 469 | Loss_D 0.4058 | Loss_G 3.9548\n",
      "Epoch 04 | Step 0100 / 469 | Loss_D 0.3972 | Loss_G 3.5973\n",
      "Epoch 04 | Step 0200 / 469 | Loss_D 0.3951 | Loss_G 3.6464\n",
      "Epoch 04 | Step 0300 / 469 | Loss_D 0.3930 | Loss_G 3.8479\n",
      "Epoch 04 | Step 0400 / 469 | Loss_D 0.3912 | Loss_G 3.9664\n",
      "Epoch 04 | Step 0469 / 469 | Loss_D 0.3903 | Loss_G 4.0033\n",
      "Epoch 05 | Step 0100 / 469 | Loss_D 0.3836 | Loss_G 3.8811\n",
      "Epoch 05 | Step 0200 / 469 | Loss_D 0.3806 | Loss_G 3.8779\n",
      "Epoch 05 | Step 0300 / 469 | Loss_D 0.3795 | Loss_G 3.9023\n",
      "Epoch 05 | Step 0400 / 469 | Loss_D 0.3785 | Loss_G 3.9299\n",
      "Epoch 05 | Step 0469 / 469 | Loss_D 0.3781 | Loss_G 3.9374\n",
      "Epoch 06 | Step 0100 / 469 | Loss_D 0.3730 | Loss_G 4.0236\n",
      "Epoch 06 | Step 0200 / 469 | Loss_D 0.3719 | Loss_G 4.0649\n",
      "Epoch 06 | Step 0300 / 469 | Loss_D 0.3714 | Loss_G 4.0863\n",
      "Epoch 06 | Step 0400 / 469 | Loss_D 0.3707 | Loss_G 4.0963\n",
      "Epoch 06 | Step 0469 / 469 | Loss_D 0.3707 | Loss_G 4.2150\n",
      "Epoch 07 | Step 0100 / 469 | Loss_D 0.3700 | Loss_G 4.9660\n",
      "Epoch 07 | Step 0200 / 469 | Loss_D 0.3685 | Loss_G 4.7003\n",
      "Epoch 07 | Step 0300 / 469 | Loss_D 0.3673 | Loss_G 4.5025\n",
      "Epoch 07 | Step 0400 / 469 | Loss_D 0.3666 | Loss_G 4.4043\n",
      "Epoch 07 | Step 0469 / 469 | Loss_D 0.3662 | Loss_G 4.3694\n",
      "Epoch 08 | Step 0100 / 469 | Loss_D 0.3638 | Loss_G 4.2397\n",
      "Epoch 08 | Step 0200 / 469 | Loss_D 0.3647 | Loss_G 4.7598\n",
      "Epoch 08 | Step 0300 / 469 | Loss_D 0.3646 | Loss_G 4.8323\n",
      "Epoch 08 | Step 0400 / 469 | Loss_D 0.3641 | Loss_G 4.8352\n",
      "Epoch 08 | Step 0469 / 469 | Loss_D 0.3638 | Loss_G 4.7992\n",
      "Epoch 09 | Step 0100 / 469 | Loss_D 0.3606 | Loss_G 4.2912\n",
      "Epoch 09 | Step 0200 / 469 | Loss_D 0.3601 | Loss_G 4.2594\n",
      "Epoch 09 | Step 0300 / 469 | Loss_D 0.3597 | Loss_G 4.2714\n",
      "Epoch 09 | Step 0400 / 469 | Loss_D 0.3593 | Loss_G 4.2770\n",
      "Epoch 09 | Step 0469 / 469 | Loss_D 0.3590 | Loss_G 4.2841\n",
      "Epoch 10 | Step 0100 / 469 | Loss_D 0.3579 | Loss_G 4.8944\n",
      "Epoch 10 | Step 0200 / 469 | Loss_D 0.3585 | Loss_G 4.9862\n",
      "Epoch 10 | Step 0300 / 469 | Loss_D 0.3586 | Loss_G 4.9854\n",
      "Epoch 10 | Step 0400 / 469 | Loss_D 0.3593 | Loss_G 5.0194\n",
      "Epoch 10 | Step 0469 / 469 | Loss_D 0.3596 | Loss_G 5.0541\n",
      "Epoch 11 | Step 0100 / 469 | Loss_D 0.3596 | Loss_G 4.9186\n",
      "Epoch 11 | Step 0200 / 469 | Loss_D 0.3590 | Loss_G 4.7197\n",
      "Epoch 11 | Step 0300 / 469 | Loss_D 0.3578 | Loss_G 4.6517\n",
      "Epoch 11 | Step 0400 / 469 | Loss_D 0.3571 | Loss_G 4.5772\n",
      "Epoch 11 | Step 0469 / 469 | Loss_D 0.3567 | Loss_G 4.5406\n",
      "Epoch 12 | Step 0100 / 469 | Loss_D 0.3534 | Loss_G 4.3303\n",
      "Epoch 12 | Step 0200 / 469 | Loss_D 0.3534 | Loss_G 4.3358\n",
      "Epoch 12 | Step 0300 / 469 | Loss_D 0.3533 | Loss_G 4.3719\n",
      "Epoch 12 | Step 0400 / 469 | Loss_D 0.3530 | Loss_G 4.3855\n",
      "Epoch 12 | Step 0469 / 469 | Loss_D 0.3531 | Loss_G 4.3873\n",
      "Epoch 13 | Step 0100 / 469 | Loss_D 0.3568 | Loss_G 5.3247\n",
      "Epoch 13 | Step 0200 / 469 | Loss_D 0.3581 | Loss_G 5.2538\n",
      "Epoch 13 | Step 0300 / 469 | Loss_D 0.3572 | Loss_G 5.1657\n",
      "Epoch 13 | Step 0400 / 469 | Loss_D 0.3568 | Loss_G 5.1223\n",
      "Epoch 13 | Step 0469 / 469 | Loss_D 0.3566 | Loss_G 5.1242\n",
      "Epoch 14 | Step 0100 / 469 | Loss_D 0.3554 | Loss_G 5.1450\n",
      "Epoch 14 | Step 0200 / 469 | Loss_D 0.3543 | Loss_G 4.9579\n",
      "Epoch 14 | Step 0300 / 469 | Loss_D 0.3535 | Loss_G 4.8120\n",
      "Epoch 14 | Step 0400 / 469 | Loss_D 0.3531 | Loss_G 4.7126\n",
      "Epoch 14 | Step 0469 / 469 | Loss_D 0.3529 | Loss_G 4.6778\n",
      "Epoch 15 | Step 0100 / 469 | Loss_D 0.3511 | Loss_G 4.5378\n",
      "Epoch 15 | Step 0200 / 469 | Loss_D 0.3507 | Loss_G 4.4847\n",
      "Epoch 15 | Step 0300 / 469 | Loss_D 0.3504 | Loss_G 4.4619\n",
      "Epoch 15 | Step 0400 / 469 | Loss_D 0.3499 | Loss_G 4.4768\n",
      "Epoch 15 | Step 0469 / 469 | Loss_D 0.3498 | Loss_G 4.4868\n",
      "Epoch 16 | Step 0100 / 469 | Loss_D 0.3482 | Loss_G 4.5859\n",
      "Epoch 16 | Step 0200 / 469 | Loss_D 0.3507 | Loss_G 5.0273\n",
      "Epoch 16 | Step 0300 / 469 | Loss_D 0.3515 | Loss_G 5.0990\n",
      "Epoch 16 | Step 0400 / 469 | Loss_D 0.3512 | Loss_G 5.0826\n",
      "Epoch 16 | Step 0469 / 469 | Loss_D 0.3512 | Loss_G 5.0711\n",
      "Epoch 17 | Step 0100 / 469 | Loss_D 0.3496 | Loss_G 4.6922\n",
      "Epoch 17 | Step 0200 / 469 | Loss_D 0.3494 | Loss_G 4.5889\n",
      "Epoch 17 | Step 0300 / 469 | Loss_D 0.3490 | Loss_G 4.5685\n",
      "Epoch 17 | Step 0400 / 469 | Loss_D 0.3484 | Loss_G 4.5623\n",
      "Epoch 17 | Step 0469 / 469 | Loss_D 0.3480 | Loss_G 4.5698\n",
      "Epoch 18 | Step 0100 / 469 | Loss_D 0.3475 | Loss_G 4.5806\n",
      "Epoch 18 | Step 0200 / 469 | Loss_D 0.3480 | Loss_G 4.8732\n",
      "Epoch 18 | Step 0300 / 469 | Loss_D 0.3487 | Loss_G 5.0696\n",
      "Epoch 18 | Step 0400 / 469 | Loss_D 0.3492 | Loss_G 5.1050\n",
      "Epoch 18 | Step 0469 / 469 | Loss_D 0.3491 | Loss_G 5.0863\n",
      "Epoch 19 | Step 0100 / 469 | Loss_D 0.3469 | Loss_G 4.6560\n",
      "Epoch 19 | Step 0200 / 469 | Loss_D 0.3468 | Loss_G 4.6725\n",
      "Epoch 19 | Step 0300 / 469 | Loss_D 0.3462 | Loss_G 4.6542\n",
      "Epoch 19 | Step 0400 / 469 | Loss_D 0.3461 | Loss_G 4.6468\n",
      "Epoch 19 | Step 0469 / 469 | Loss_D 0.3461 | Loss_G 4.6664\n",
      "Epoch 20 | Step 0100 / 469 | Loss_D 0.3457 | Loss_G 4.6858\n",
      "Epoch 20 | Step 0200 / 469 | Loss_D 0.3455 | Loss_G 4.6700\n",
      "Epoch 20 | Step 0300 / 469 | Loss_D 0.3455 | Loss_G 4.6665\n",
      "Epoch 20 | Step 0400 / 469 | Loss_D 0.3452 | Loss_G 4.6657\n",
      "Epoch 20 | Step 0469 / 469 | Loss_D 0.3451 | Loss_G 4.6708\n",
      "Epoch 21 | Step 0100 / 469 | Loss_D 0.3444 | Loss_G 4.7299\n",
      "Epoch 21 | Step 0200 / 469 | Loss_D 0.3443 | Loss_G 4.7452\n",
      "Epoch 21 | Step 0300 / 469 | Loss_D 0.3443 | Loss_G 4.7146\n",
      "Epoch 21 | Step 0400 / 469 | Loss_D 0.3443 | Loss_G 4.7230\n",
      "Epoch 21 | Step 0469 / 469 | Loss_D 0.3447 | Loss_G 4.8413\n",
      "Epoch 22 | Step 0100 / 469 | Loss_D 0.3504 | Loss_G 5.8105\n",
      "Epoch 22 | Step 0200 / 469 | Loss_D 0.3494 | Loss_G 5.6884\n",
      "Epoch 22 | Step 0300 / 469 | Loss_D 0.3492 | Loss_G 5.5499\n",
      "Epoch 22 | Step 0400 / 469 | Loss_D 0.3494 | Loss_G 5.5768\n",
      "Epoch 22 | Step 0469 / 469 | Loss_D 0.3494 | Loss_G 5.5800\n",
      "Epoch 23 | Step 0100 / 469 | Loss_D 0.3504 | Loss_G 5.6988\n",
      "Epoch 23 | Step 0200 / 469 | Loss_D 0.3493 | Loss_G 5.5363\n",
      "Epoch 23 | Step 0300 / 469 | Loss_D 0.3486 | Loss_G 5.4339\n",
      "Epoch 23 | Step 0400 / 469 | Loss_D 0.3483 | Loss_G 5.4041\n",
      "Epoch 23 | Step 0469 / 469 | Loss_D 0.3480 | Loss_G 5.3828\n",
      "Epoch 24 | Step 0100 / 469 | Loss_D 0.3474 | Loss_G 5.2050\n",
      "Epoch 24 | Step 0200 / 469 | Loss_D 0.3475 | Loss_G 5.2331\n",
      "Epoch 24 | Step 0300 / 469 | Loss_D 0.3475 | Loss_G 5.2415\n",
      "Epoch 24 | Step 0400 / 469 | Loss_D 0.3477 | Loss_G 5.2892\n",
      "Epoch 24 | Step 0469 / 469 | Loss_D 0.3477 | Loss_G 5.2827\n",
      "Epoch 25 | Step 0100 / 469 | Loss_D 0.3506 | Loss_G 5.3216\n",
      "Epoch 25 | Step 0200 / 469 | Loss_D 0.3501 | Loss_G 5.2969\n",
      "Epoch 25 | Step 0300 / 469 | Loss_D 0.3497 | Loss_G 5.2858\n",
      "Epoch 25 | Step 0400 / 469 | Loss_D 0.3494 | Loss_G 5.2540\n",
      "Epoch 25 | Step 0469 / 469 | Loss_D 0.3489 | Loss_G 5.2278\n",
      "Epoch 26 | Step 0100 / 469 | Loss_D 0.3457 | Loss_G 4.7564\n",
      "Epoch 26 | Step 0200 / 469 | Loss_D 0.3452 | Loss_G 4.7069\n",
      "Epoch 26 | Step 0300 / 469 | Loss_D 0.3448 | Loss_G 4.6889\n",
      "Epoch 26 | Step 0400 / 469 | Loss_D 0.3444 | Loss_G 4.6891\n",
      "Epoch 26 | Step 0469 / 469 | Loss_D 0.3442 | Loss_G 4.6966\n",
      "Epoch 27 | Step 0100 / 469 | Loss_D 0.3433 | Loss_G 4.8038\n",
      "Epoch 27 | Step 0200 / 469 | Loss_D 0.3435 | Loss_G 4.8295\n",
      "Epoch 27 | Step 0300 / 469 | Loss_D 0.3433 | Loss_G 4.8540\n",
      "Epoch 27 | Step 0400 / 469 | Loss_D 0.3427 | Loss_G 4.8795\n",
      "Epoch 27 | Step 0469 / 469 | Loss_D 0.3425 | Loss_G 4.8832\n",
      "Epoch 28 | Step 0100 / 469 | Loss_D 0.3424 | Loss_G 4.9011\n",
      "Epoch 28 | Step 0200 / 469 | Loss_D 0.3419 | Loss_G 4.8931\n",
      "Epoch 28 | Step 0300 / 469 | Loss_D 0.3418 | Loss_G 4.8847\n",
      "Epoch 28 | Step 0400 / 469 | Loss_D 0.3417 | Loss_G 4.8712\n",
      "Epoch 28 | Step 0469 / 469 | Loss_D 0.3415 | Loss_G 4.8691\n",
      "Epoch 29 | Step 0100 / 469 | Loss_D 0.3408 | Loss_G 4.8429\n",
      "Epoch 29 | Step 0200 / 469 | Loss_D 0.3409 | Loss_G 4.8408\n",
      "Epoch 29 | Step 0300 / 469 | Loss_D 0.3410 | Loss_G 4.8498\n",
      "Epoch 29 | Step 0400 / 469 | Loss_D 0.3412 | Loss_G 4.8597\n",
      "Epoch 29 | Step 0469 / 469 | Loss_D 0.3410 | Loss_G 4.8813\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
