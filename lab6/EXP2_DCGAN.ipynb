{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8527ab-86db-4349-8391-3b08490979f9",
   "metadata": {},
   "source": [
    "### 请仔细阅读DCGAN相关材料并补充完整下面的代码。在需要补充的部分已经标注#TODO并附上相应的内容提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1000fa-3982-4593-93ef-df9ba98aaf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import torch  # PyTorch 深度学习框架\n",
    "import torch.nn as nn  # 神经网络相关模块\n",
    "import numpy as np  # 数值计算库\n",
    "from torch.utils.data import DataLoader  # 处理数据加载\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms  # 处理图像数据集和数据变换\n",
    "from torchvision.utils import save_image  # 保存生成的图像\n",
    "import os  # 处理文件和目录操作\n",
    "from torch.utils.tensorboard import SummaryWriter  # TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a810b93e-400f-4dde-b7a3-529ac3a0a793",
   "metadata": {},
   "source": [
    "#### 根据文档和提示，参考GAN的实现代码，补充完整DCGAN的生成器Generator和判别器Discriminator代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "422f176d-31ab-4674-bcab-4216578f6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== 生成器（Generator） ===============================\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # 1. 输入层：将 100 维随机噪声投影到 32x32（1024 维）\n",
    "        #TODO   # 线性变换fc1，将输入噪声扩展到 1024 维\n",
    "        self.fc1 = nn.Linear(input_dim, 32*32)\n",
    "        self.br1 = nn.Sequential(\n",
    "            #TODO   # 批归一化，加速训练并稳定收敛\n",
    "            nn.BatchNorm1d(32*32),\n",
    "            #TODO   # ReLU 激活函数，引入非线性\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 2. 第二层：将 1024 维数据映射到 128 * 7 * 7 维特征\n",
    "        #TODO   # 线性变换fc2，将数据变换为适合卷积层的维数大小\n",
    "        self.fc2 = nn.Linear(32*32, 128*7*7)\n",
    "        self.br2 = nn.Sequential(\n",
    "            #TODO   # 批归一化\n",
    "            nn.BatchNorm1d(128*7*7),\n",
    "            #TODO   # ReLU 激活函数\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 3. 反卷积层 1：上采样，输出 64 通道的 14×14 特征图\n",
    "        self.conv1 = nn.Sequential(\n",
    "            #TODO   # 反卷积：将 7x7 放大到 14x14，kernel size设置为4，stride设置为2，padding设置为1\n",
    "            nn.ConvTranspose2d(in_channels=128 ,out_channels=64,kernel_size=4, stride=2, padding=1),\n",
    "            #TODO   # 归一化，稳定训练\n",
    "            nn.BatchNorm2d(64),\n",
    "            #TODO   # ReLU 激活函数\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 4. 反卷积层 2：输出 1 通道的 28×28 图像\n",
    "        self.conv2 = nn.Sequential(\n",
    "            #TODO   # 反卷积：将 14x14 放大到 28x28，将 7x7 放大到 14x14，kernel size设置为4，stride设置为2，padding设置为1\n",
    "            nn.ConvTranspose2d(in_channels=64 ,out_channels=1 ,kernel_size=4, stride=2, padding=1),\n",
    "            #TODO   # Sigmoid 激活函数，将输出归一化到 [0,1]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.br1(self.fc1(x))  # 通过全连接层，进行 BatchNorm 和 ReLU 激活\n",
    "        x = self.br2(self.fc2(x))  # 继续通过全连接层，进行 BatchNorm 和 ReLU 激活\n",
    "        x = x.reshape(-1, 128, 7, 7)  # 变形为适合卷积输入的形状 (batch, 128, 7, 7)\n",
    "        x = self.conv1(x)  # 反卷积：上采样到 14x14\n",
    "        output = self.conv2(x)  # 反卷积：上采样到 28x28\n",
    "        return output  # 返回生成的图像\n",
    "\n",
    "# =============================== 判别器（Discriminator） ===============================\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # 1. 第一层：输入 1 通道的 28x28 图像，输出 32 通道的特征图，然后通过MaxPool2d降采样\n",
    "        self.conv1 = nn.Sequential(\n",
    "            #TODO  # 5x5 卷积核，步长为1\n",
    "            nn.Conv2d(in_channels=1, out_channels=32 ,kernel_size=5, stride=1),\n",
    "            #TODO   # LeakyReLU，negative_slope参数设置为0.1\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.pl1 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        # 2. 第二层：输入 32 通道，输出 64 通道特征, 然后通过MaxPool2d降采样\n",
    "        self.conv2 = nn.Sequential(\n",
    "            #TODO   # 5x5 卷积核，步长为1\n",
    "            nn.Conv2d(in_channels=32, out_channels=64 ,kernel_size=5, stride=1),\n",
    "            #TODO  # LeakyReLU 激活函数，negative_slope参数设置为0.1\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.pl2 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        # 3. 全连接层 1：将 64x4x4 维特征图转换成 1024 维向量\n",
    "        self.fc1 = nn.Sequential(\n",
    "            #TODO   # 线性变换，将 64x4x4 映射到 1024 维\n",
    "            nn.Linear(64*4*4, 1024),\n",
    "            #TODO   # LeakyReLU 激活函数，negative_slope参数设置为0.1\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "\n",
    "        # 4. 全连接层 2：最终输出真假概率\n",
    "        self.fc2 = nn.Sequential(\n",
    "            #TODO   # 线性变换，将 1024 维数据映射到 1 维\n",
    "            nn.Linear(1024,1),\n",
    "            #TODO   # Sigmoid 归一化到 [0,1] 作为概率输出\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pl1(self.conv1(x))  # 第一层卷积，降维\n",
    "        x = self.pl2(self.conv2(x))  # 第二层卷积，降维\n",
    "        x = x.view(x.shape[0], -1)  # 展平成向量\n",
    "        x = self.fc1(x)  # 通过全连接层\n",
    "        output = self.fc2(x)  # 通过最后一层全连接层，输出真假概率\n",
    "        return output  # 返回判别结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cdc92b-007d-4825-85d5-478280ab5989",
   "metadata": {},
   "source": [
    "#### 补充完整主函数，在主函数中完成以下过程：\n",
    "1. 数据加载：\n",
    "加载并预处理数据集。对于DCGAN的训练，通常需要将数据集转换为张量格式，并进行适当的归一化。\n",
    "2. 模型初始化：\n",
    "创建生成器和判别器模型实例，并将它们移动到合适的设备（如GPU）上。\n",
    "3. 优化器和损失函数定义：\n",
    "为生成器和判别器分别定义优化器（如Adam），并设置适当的学习率和其他超参数。\n",
    "定义损失函数（如二元交叉熵损失）用于评估模型性能。\n",
    "4. 训练循环：\n",
    "迭代多个epoch进行训练。在每个epoch中，遍历数据集并进行以下操作：\n",
    "   * 训练判别器：使用真实数据和生成的假数据更新判别器的参数。\n",
    "   * 训练生成器：通过生成假数据并试图欺骗判别器来更新生成器的参数。\n",
    "   * 记录损失值到TensorBoard，以监控训练过程。\n",
    "5. 结果保存：\n",
    "在每个epoch结束时，生成一些示例图像并保存到TensorBoard，以便观察生成器的进展。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33b878c4-5f0c-4fd0-b482-ee931e176837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== 主函数 ===============================\n",
    "def main():\n",
    "    # 设备配置：使用 GPU（如果可用），否则使用 CPU\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 设定超参数\n",
    "    input_dim = 100  # 生成器输入的随机噪声向量维度\n",
    "    batch_size = 128  # 训练时的批量大小\n",
    "    num_epoch = 10  # 训练的总轮数\n",
    "\n",
    "    # 加载 MNIST 数据集\n",
    "    train_dataset = datasets.MNIST(root=\"./data/\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 创建生成器和判别器，并移动到 GPU（如果可用）\n",
    "    # TODO\n",
    "    G = Generator(input_dim).to(device)\n",
    "    # TODO\n",
    "    D = Discriminator().to(device)\n",
    "\n",
    "    # 定义优化器，优化器要求同任务一\n",
    "    # TODO\n",
    "    optim_G = torch.optim.Adam(G.parameters(), lr=0.0002)\n",
    "    # TODO\n",
    "    optim_D = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "    loss_func = nn.BCELoss()\n",
    "\n",
    "    # 初始化 TensorBoard\n",
    "    writer = SummaryWriter(log_dir='./logs/experiment_dcgan')\n",
    "\n",
    "    # 开始训练\n",
    "    for epoch in range(num_epoch):\n",
    "        total_loss_D, total_loss_G = 0, 0\n",
    "        for i, (real_images, _) in enumerate(train_loader):\n",
    "            loss_D = train_discriminator(real_images, D, G, loss_func, optim_D, batch_size, input_dim, device)\n",
    "            loss_G = train_generator(D, G, loss_func, optim_G, batch_size, input_dim, device)\n",
    "\n",
    "            total_loss_D += loss_D\n",
    "            total_loss_G += loss_G\n",
    "\n",
    "            # 每 100 步打印一次损失\n",
    "            if (i + 1) % 100 == 0 or (i + 1) == len(train_loader):\n",
    "                print(f'Epoch {epoch:02d} | Step {i + 1:04d} / {len(train_loader)} | Loss_D {total_loss_D / (i + 1):.4f} | Loss_G {total_loss_G / (i + 1):.4f}')\n",
    "\n",
    "        # 记录损失到 TensorBoard\n",
    "        writer.add_scalar('DCGAN/Loss/Discriminator', total_loss_D / len(train_loader), epoch)\n",
    "        writer.add_scalar('DCGAN/Loss/Generator', total_loss_G / len(train_loader), epoch)\n",
    "\n",
    "        # 生成并保存示例图像\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(64, input_dim, device=device)\n",
    "            fake_images = G(noise)\n",
    "\n",
    "            # 记录生成的图像到 TensorBoard\n",
    "            img_grid = torchvision.utils.make_grid(fake_images, normalize=True)\n",
    "            writer.add_image('Generated Images', img_grid, epoch)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e9643-842e-4595-a4bf-ddf45fd6d979",
   "metadata": {},
   "source": [
    "#### 根据文档中描述的GAN的损失函数和二元交叉熵损失相关内容，补充完善Discriminator和Generator的训练过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cedb8fe6-3674-4535-946e-5dfb06d836fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== 训练判别器 ===============================\n",
    "def train_discriminator(real_images, D, G, loss_func, optim_D, batch_size, input_dim, device):\n",
    "    '''训练判别器'''\n",
    "    real_output = D(real_images)  # 判别器预测真实图像\n",
    "    real_target = real_images.mean(dim = [2,3])\n",
    "    #TODO   # 计算真实样本的损失real_loss\n",
    "    real_loss = loss_func(real_output, real_target)\n",
    "    noise = torch.randn(real_images.shape[0], input_dim, device=device)  # 生成随机噪声\n",
    "    fake_images = G(noise).detach()  # 生成假图像（detach 避免梯度传递给 G）\n",
    "    fake_output = D(fake_images)  # 判别器预测假图像\n",
    "    \n",
    "    #TODO   # 计算假样本的损失fake_loss\n",
    "    fake_loss = loss_func(fake_output, real_target)\n",
    "\n",
    "    loss_D = real_loss + fake_loss  # 判别器总损失\n",
    "    optim_D.zero_grad()  # 清空梯度\n",
    "    loss_D.backward()  # 反向传播\n",
    "    optim_D.step()  # 更新判别器参数\n",
    "\n",
    "    return loss_D.item()  # 返回标量损失\n",
    "\n",
    "\n",
    "# =============================== 训练生成器 ===============================\n",
    "def train_generator(D, G, loss_func, optim_G, batch_size, input_dim, device):\n",
    "    '''训练生成器'''\n",
    "    noise = torch.randn(batch_size, input_dim, device=device)  # 生成随机噪声\n",
    "    fake_images = G(noise)  # 生成假图像\n",
    "    fake_output = D(fake_images)  # 判别器对假图像的判断\n",
    "    #TODO # 计算生成器损失（希望生成的图像判别为真）\n",
    "    loss_G = loss_func(fake_output, torch.ones(batch_size,1))\n",
    "    optim_G.zero_grad()  # 清空梯度\n",
    "    loss_G.backward()  # 反向传播\n",
    "    optim_G.step()  # 更新生成器参数\n",
    "\n",
    "    return loss_G.item()  # 返回标量损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d78a3-c042-4ddf-8459-ffcd19588c6d",
   "metadata": {},
   "source": [
    "#### 主函数执行入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04ecaee8-7278-4e9a-8c49-5f10b7800063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Step 0100 / 469 | Loss_D 0.8070 | Loss_G 1.8988\n",
      "Epoch 00 | Step 0200 / 469 | Loss_D 0.7895 | Loss_G 1.9693\n",
      "Epoch 00 | Step 0300 / 469 | Loss_D 0.7821 | Loss_G 1.9930\n",
      "Epoch 00 | Step 0400 / 469 | Loss_D 0.7783 | Loss_G 2.0045\n",
      "Epoch 00 | Step 0469 / 469 | Loss_D 0.7768 | Loss_G 2.0093\n",
      "Epoch 01 | Step 0100 / 469 | Loss_D 0.7683 | Loss_G 2.0351\n",
      "Epoch 01 | Step 0200 / 469 | Loss_D 0.7675 | Loss_G 2.0331\n",
      "Epoch 01 | Step 0300 / 469 | Loss_D 0.7680 | Loss_G 2.0261\n",
      "Epoch 01 | Step 0400 / 469 | Loss_D 0.7687 | Loss_G 2.0187\n",
      "Epoch 01 | Step 0469 / 469 | Loss_D 0.7692 | Loss_G 2.0145\n",
      "Epoch 02 | Step 0100 / 469 | Loss_D 0.7710 | Loss_G 1.9901\n",
      "Epoch 02 | Step 0200 / 469 | Loss_D 0.7700 | Loss_G 1.9882\n",
      "Epoch 02 | Step 0300 / 469 | Loss_D 0.7703 | Loss_G 1.9859\n",
      "Epoch 02 | Step 0400 / 469 | Loss_D 0.7702 | Loss_G 1.9844\n",
      "Epoch 02 | Step 0469 / 469 | Loss_D 0.7700 | Loss_G 1.9844\n",
      "Epoch 03 | Step 0100 / 469 | Loss_D 0.7695 | Loss_G 1.9893\n",
      "Epoch 03 | Step 0200 / 469 | Loss_D 0.7677 | Loss_G 1.9892\n",
      "Epoch 03 | Step 0300 / 469 | Loss_D 0.7694 | Loss_G 1.9810\n",
      "Epoch 03 | Step 0400 / 469 | Loss_D 0.7698 | Loss_G 1.9788\n",
      "Epoch 03 | Step 0469 / 469 | Loss_D 0.7701 | Loss_G 1.9762\n",
      "Epoch 04 | Step 0100 / 469 | Loss_D 0.7678 | Loss_G 1.9703\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[37], line 36\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (real_images, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     35\u001b[0m     loss_D \u001b[38;5;241m=\u001b[39m train_discriminator(real_images, D, G, loss_func, optim_D, batch_size, input_dim, device)\n\u001b[0;32m---> 36\u001b[0m     loss_G \u001b[38;5;241m=\u001b[39m train_generator(D, G, loss_func, optim_G, batch_size, input_dim, device)\n\u001b[1;32m     38\u001b[0m     total_loss_D \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_D\n\u001b[1;32m     39\u001b[0m     total_loss_G \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_G\n",
      "Cell \u001b[0;32mIn[52], line 32\u001b[0m, in \u001b[0;36mtrain_generator\u001b[0;34m(D, G, loss_func, optim_G, batch_size, input_dim, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m loss_G \u001b[38;5;241m=\u001b[39m loss_func(fake_output, torch\u001b[38;5;241m.\u001b[39mones(batch_size,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     31\u001b[0m optim_G\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 清空梯度\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m loss_G\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[1;32m     33\u001b[0m optim_G\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# 更新生成器参数\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_G\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
